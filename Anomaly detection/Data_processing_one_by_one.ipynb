{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ba46a90-36a2-4014-97f5-92fbb49dd731",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import dask.dataframe as dd\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import torch\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "from collections import Counter\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ff12e5c-3847-4a9d-824e-959e600c7d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_attack_name_list = [('/home/binghui/NDSS2025/Intrution-detection/CICdataset2018/Botnet/Botnet-attack.csv', 1), ('/home/binghui/NDSS2025/Intrution-detection/CICdataset2018/BruteForce/BruteForce-attack.csv', 2),  ('/home/binghui/NDSS2025/Intrution-detection/CICdataset2018/DDoS/DDoS-attack.csv', 3), ('/home/binghui/NDSS2025/Intrution-detection/CICdataset2018/DoS/DoS-attack.csv', 4), ('/home/binghui/NDSS2025/Intrution-detection/CICdataset2018/Infiltration/Infiltration-attack.csv', 5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f9e9fa81-6068-4049-a1b2-c185befbd599",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_benign_name_list = [('/home/binghui/NDSS2025/Intrution-detection/CICdataset2018/Botnet/Botnet-benign.csv', 0), ('/home/binghui/NDSS2025/Intrution-detection/CICdataset2018/BruteForce/BruteForce-benign.csv', 0), ('/home/binghui/NDSS2025/Intrution-detection/CICdataset2018/DDoS/DDoS-benign.csv', 0),  ('/home/binghui/NDSS2025/Intrution-detection/CICdataset2018/DoS/DoS-benign.csv', 0), ('/home/binghui/NDSS2025/Intrution-detection/CICdataset2018/Infiltration/Infiltration-benign.csv', 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "4902d9a4-94ed-40b3-9fc6-0d756d66c9b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = '/home/binghui/NDSS2025/Anomaly-detection/CICdataset2018/output-1000/Infiltration-attack.pkl'\n",
    "file_name = '/home/binghui/NDSS2025/Intrution-detection/CICdataset2018/Infiltration/Infiltration-attack.csv'\n",
    "label = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "797dca37-b2ba-4240-9a57-54174c77df72",
   "metadata": {},
   "outputs": [],
   "source": [
    "## help functions\n",
    "# Define a function to modify 'ServePort' based on the condition\n",
    "def modify_serve_port(df):\n",
    "    df['ServePort'] = np.where(df['ServePort'] > 1024, 1025, df['ServePort'])\n",
    "    return df\n",
    "\n",
    "def assign_flow_id(df):\n",
    "    # Function to create a sorted string from IPs and Ports\n",
    "    def create_flow_identifier(row):\n",
    "        # Sort IPs\n",
    "        ips = sorted([row['IP_source'], row['IP_destination']])\n",
    "        # Sort Ports\n",
    "        ports = sorted([row['Port_source'], row['Port_destination']])\n",
    "        # Combine with Layer 4 protocol\n",
    "        return f\"{ips[0]}_{ips[1]}_{ports[0]}_{ports[1]}_{row['Layer_4_protocol']}\"\n",
    "\n",
    "    # Apply the function to each row to create a flow identifier\n",
    "    df['flow_identifier'] = df.apply(create_flow_identifier, axis=1)\n",
    "\n",
    "    # Assign flow ID based on the unique flow identifiers\n",
    "    df['flow_id'] = pd.factorize(df['flow_identifier'])[0]\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def create_flow_summary(df):\n",
    "    # Group by flow_id and aggregate\n",
    "    df['serve_ip'] = df.apply(lambda x: min(x['IP_source'], x['IP_destination']), axis=1)\n",
    "\n",
    "    flow_summary = df.groupby('flow_id').agg(\n",
    "        serve_ip=pd.NamedAgg(column='serve_ip', aggfunc='first'),  # Include serve_ip in the summary\n",
    "        start_time=pd.NamedAgg(column='TIME', aggfunc='min'),\n",
    "        end_time=pd.NamedAgg(column='TIME', aggfunc='max'),\n",
    "        average_packet_size=pd.NamedAgg(column='Size', aggfunc='mean'),\n",
    "        average_IAT=pd.NamedAgg(column='IAT', aggfunc='mean'),\n",
    "        num_packets=pd.NamedAgg(column='TIME', aggfunc='count'),\n",
    "        l4_protocol = pd.NamedAgg(column='Layer_4_protocol', aggfunc='first'),\n",
    "        srcIP=pd.NamedAgg(column='IP_source', aggfunc='first'),  # Extract the Source IP\n",
    "        dstIP=pd.NamedAgg(column='IP_destination', aggfunc='first'),  # Extract the Destination IP\n",
    "        ServePort = pd.NamedAgg(column='ServePort', aggfunc='first')\n",
    "    )\n",
    "\n",
    "    # Calculate duration\n",
    "    flow_summary['duration'] = flow_summary['end_time'] - flow_summary['start_time']\n",
    "    # Determine if flow is bidirectional\n",
    "    flow_summary['direction'] = df.groupby('flow_id').apply(\n",
    "        lambda x: 1 if len(x['IP_source'].unique()) > 1 else 0\n",
    "    )\n",
    "    flow_summary = flow_summary.reset_index()\n",
    "    return flow_summary\n",
    "\n",
    "last_end_time = {}\n",
    "global_session_id = 0\n",
    "# Function to assign session IDs\n",
    "def assign_session_id(row):\n",
    "    global global_session_id\n",
    "\n",
    "    serve_ip = row['serve_ip']\n",
    "    start_time = row['start_time']\n",
    "    \n",
    "    # Initialize if this is the first flow for the serve_ip\n",
    "    if serve_ip not in last_end_time:\n",
    "        last_end_time[serve_ip] = row['end_time']\n",
    "        global_session_id += 1\n",
    "        return global_session_id\n",
    "    \n",
    "    # Calculate inter-time\n",
    "    inter_time = (start_time - last_end_time[serve_ip]).total_seconds()\n",
    "    \n",
    "    # Check if new session should start\n",
    "    if inter_time > 30:\n",
    "        global_session_id += 1\n",
    "    \n",
    "    # Update the last end time\n",
    "    last_end_time[serve_ip] = row['end_time']\n",
    "    return global_session_id\n",
    "\n",
    "\n",
    "## help functions\n",
    "### Tokenization Binning\n",
    "# process for the Packet\n",
    "def equal_width_binning_packet(df):\n",
    "    n_bins = 1026\n",
    "    strategy = 'uniform' # quantile for equal-frequency, kmeans for k-clustering\n",
    "    subsample_size = 20000  # Set this to None to disable subsampling\n",
    "    scalers = {\n",
    "        'IAT': KBinsDiscretizer(n_bins=n_bins, encode='ordinal', strategy=strategy, subsample=subsample_size),\n",
    "        'Size': KBinsDiscretizer(n_bins=n_bins, encode='ordinal', strategy=strategy, subsample=subsample_size),\n",
    "        'Payload_Size': KBinsDiscretizer(n_bins=n_bins, encode='ordinal', strategy=strategy, subsample=subsample_size)  \n",
    "    }\n",
    "\n",
    "    df_copy = df.copy()\n",
    "    for feature, scaler in scalers.items():\n",
    "        scaler.fit(df_copy[[feature]])\n",
    "        df_copy[feature] = scaler.transform(df_copy[[feature]]).astype(int)\n",
    "    return df_copy\n",
    "\n",
    "# process for the flow\n",
    "def equal_width_binning_flow(df):\n",
    "    n_bins = 1026\n",
    "    strategy = 'uniform' # quantile for equal-frequency, kmeans for k-clustering\n",
    "    subsample_size = 20000  # Set this to None to disable subsampling\n",
    "    scaler = KBinsDiscretizer(n_bins=n_bins, encode='ordinal', strategy=strategy, subsample=subsample_size)\n",
    "\n",
    "    df_copy = df.copy()\n",
    "    columns_to_bin = ['duration','average_packet_size','average_IAT','num_packets']\n",
    "    \n",
    "    for col in columns_to_bin:\n",
    "        scaler.fit(df_copy[[col]])\n",
    "        df_copy[col] = scaler.transform(df_copy[[col]]).astype(int)\n",
    "    return df_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "1a89d703-3246-4764-a0f3-db680902f8df",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dd.read_csv(file_name)\n",
    "df['Direction'] = (df['IP_source'] < df['IP_destination']).astype(int)\n",
    "df['ServePort'] = df[['Port_source', 'Port_destination']].min(axis=1)\n",
    "\n",
    "# Apply the serve port function\n",
    "df = df.map_partitions(modify_serve_port, meta=df)\n",
    "dask_df = df.copy()\n",
    "dask_df['Flow'] = dask_df['IP_source'] + dask_df['IP_destination'] + dask_df['Port_source'].astype(str) + dask_df['Port_destination'].astype(str) + dask_df['Layer_4_protocol'].astype(str)\n",
    "dask_df['Inverse_Flow'] = dask_df['IP_destination'] + dask_df['IP_source'] + dask_df['Port_destination'].astype(str) + dask_df['Port_source'].astype(str)  + dask_df['Layer_4_protocol'].astype(str)\n",
    "## change from dask df to pandas\n",
    "df = dask_df.compute()\n",
    "df = assign_flow_id(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "e6d09b85-6116-4434-b072-5192b69ae323",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total memory: 62.62 GB\n",
      "Available memory: 58.44 GB\n",
      "Used memory: 3.51 GB\n",
      "Percentage used: 6.7%\n"
     ]
    }
   ],
   "source": [
    "import psutil\n",
    "\n",
    "# Get the memory details\n",
    "memory = psutil.virtual_memory()\n",
    "\n",
    "# Print the total, available, and used memory\n",
    "print(f\"Total memory: {memory.total / (1024**3):.2f} GB\")\n",
    "print(f\"Available memory: {memory.available / (1024**3):.2f} GB\")\n",
    "print(f\"Used memory: {memory.used / (1024**3):.2f} GB\")\n",
    "print(f\"Percentage used: {memory.percent}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "480fd5a5-d528-4981-bf25-0d552454dae8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Packet Processing Done\n",
      "(2, 13)\n",
      "Tokenization: Binning\n",
      "Tokenization: Binning done\n"
     ]
    }
   ],
   "source": [
    "# Sort by flow_id and TIME\n",
    "df = df.sort_values(by=['flow_id', 'TIME'])\n",
    "# Calculate Inter-Arrival Time within each flow\n",
    "df['IAT'] = df.groupby('flow_id')['TIME'].diff().fillna(0)\n",
    "\n",
    "## filter the huge size packet\n",
    "df = df[df['Size'] <= 5000]\n",
    "df = df[df['Payload_Size']<= 5000]\n",
    "print(\"Packet Processing Done\")\n",
    "# Create the flow-level summary DataFrame\n",
    "flow_summary_df = create_flow_summary(df)\n",
    "\n",
    "flow_df = flow_summary_df.copy()\n",
    "print(flow_df.shape)\n",
    "\n",
    "# Assuming flow_summary_df is your DataFrame\n",
    "# Convert 'start_time' and 'end_time' to datetime if they are not already\n",
    "flow_summary_df['start_time'] = pd.to_datetime(flow_summary_df['start_time'])\n",
    "flow_summary_df['end_time'] = pd.to_datetime(flow_summary_df['end_time'])\n",
    "\n",
    "# Sort by serve_ip and start_time\n",
    "flow_summary_df = flow_summary_df.sort_values(by=['serve_ip', 'start_time'])\n",
    "\n",
    "# Initialize a dictionary to track the last end time for each serve_ip\n",
    "# and a global session ID counter\n",
    "\n",
    "\n",
    "# Apply function to each row\n",
    "flow_summary_df['session_id'] = flow_summary_df.apply(assign_session_id, axis=1)\n",
    "\n",
    "\n",
    "df_with_sessions = pd.merge(df, flow_summary_df[['flow_id', 'session_id']], on='flow_id', how='left')\n",
    "df_with_sessions = df_with_sessions.sort_values(by=['session_id'])\n",
    "## final flow representation \n",
    "final_flow = flow_summary_df[['session_id','flow_id','direction', 'duration', 'l4_protocol', 'average_packet_size','average_IAT','num_packets','ServePort']].copy()\n",
    "## final packet representation\n",
    "final_packet = df_with_sessions[['session_id','flow_id','Direction', 'Layer_4_protocol','Size','Payload_Size','IAT', 'ServePort']].copy()\n",
    "\n",
    "print(\"Tokenization: Binning\")\n",
    "df_packet_token = equal_width_binning_packet(final_packet)\n",
    "df_flow_token = equal_width_binning_flow(final_flow)\n",
    "del final_packet,final_flow, df_with_sessions,flow_summary_df\n",
    "\n",
    "print(\"Tokenization: Binning done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "cf44515b-d83c-43cd-af3a-9d8a0c7d53f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## part 2 input generation \n",
    "def create_embeddings(length, embedding_value):\n",
    "    return np.full(length, embedding_value)\n",
    "\n",
    "def input_generation(df_flow_token, df_packet_token, def_label):\n",
    "    max_len = 1000\n",
    "    padding_value = 0\n",
    "    \n",
    "    session_dict = df_flow_token.groupby('session_id')['flow_id'].apply(list).to_dict()\n",
    "    flow_id_dict = df_packet_token.groupby('flow_id').apply(lambda x: x.drop(['flow_id', 'session_id'], axis=1).values.flatten().tolist()).to_dict()\n",
    "    for key in flow_id_dict.keys():\n",
    "        if len(flow_id_dict[key]) >= 6*9: ## more than 9 packet\n",
    "            flow_id_dict[key] = flow_id_dict[key][:54]  \n",
    "    flow_feature_dict = df_flow_token.groupby('flow_id').apply(lambda x: x.drop(['flow_id', 'session_id'], axis=1).values.flatten().tolist()).to_dict()\n",
    "\n",
    "\n",
    "    # Use the session_dict to create the final dictionary\n",
    "    session_feature_dict = {}\n",
    "    segment_embedding_dict = {}\n",
    "    \n",
    "    # Wrap the outer loop with tqdm for the progress bar\n",
    "    for session_id in tqdm(session_dict.keys(), desc=\"Processing sessions\"):\n",
    "        session_features = []\n",
    "        session_embeddings = []\n",
    "        for flow_id in session_dict[session_id]:\n",
    "            # Retrieve flow features and create flow embeddings\n",
    "            flow_feat = flow_feature_dict[flow_id]\n",
    "            flow_embedding = create_embeddings(len(flow_feat), 0)  # 0 for flow features\n",
    "            session_features.extend(flow_feat)\n",
    "            session_embeddings.extend(flow_embedding)\n",
    "    \n",
    "            # Retrieve packet features and create packet embeddings\n",
    "            packet_feat = flow_id_dict[flow_id]\n",
    "            packet_embedding = create_embeddings(len(packet_feat), 1)  # 1 for packet features\n",
    "            session_features.extend(packet_feat)\n",
    "            session_embeddings.extend(packet_embedding)\n",
    "        \n",
    "        session_feature_dict[session_id] =  session_features\n",
    "        segment_embedding_dict[session_id] = session_embeddings\n",
    "    \n",
    "    print(\"Processing complete.\")\n",
    "\n",
    "\n",
    "\n",
    "    input_sequences = []\n",
    "    input_segments = []\n",
    "    \n",
    "    def pad_sequence(seq, target_len, pad_value=0):\n",
    "        return np.pad(seq, (0, max(target_len - len(seq), 0)), mode='constant', constant_values=pad_value)\n",
    "    \n",
    "    for s_id in tqdm(session_feature_dict.keys(), desc=\"Processing sessions\"):\n",
    "        ids = np.array(session_feature_dict[s_id])  # Convert to numpy array for memory efficiency\n",
    "        segs = np.array(segment_embedding_dict[s_id])  # Convert to numpy array for memory efficiency\n",
    "    \n",
    "        # Splitting and padding the sequences\n",
    "        for i in range(0, len(ids), max_len):\n",
    "            end_idx = min(i + max_len, len(ids))\n",
    "            pieces = pad_sequence(ids[i:end_idx], max_len, padding_value)\n",
    "            seg_pieces = pad_sequence(segs[i:end_idx], max_len, padding_value)\n",
    "    \n",
    "            input_sequences.append(pieces)\n",
    "            input_segments.append(seg_pieces)\n",
    "    \n",
    "    print(\"Processing complete.\")\n",
    "  \n",
    "    labels = def_label * np.ones((len(input_sequences)))\n",
    "\n",
    "    return input_sequences,input_segments,labels \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "c06828e2-2959-4f7e-962e-f87672257333",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetformerDatasetDownstream(Dataset):\n",
    "    def __init__(self, input_sequences, input_segments, input_labels, seq_len = 1000):\n",
    "        self.seq_len = seq_len\n",
    "        self.session_flows = len(input_sequences)\n",
    "        self.sessions = input_sequences\n",
    "        self.segments = input_segments\n",
    "        self.labels = input_labels\n",
    "        self.special_token_dict =  {'PAD': 1027, 'MASK': 1028}\n",
    "        self.mask_ratio = 0.4\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.session_flows\n",
    "\n",
    "    def __getitem__(self,item):\n",
    "\n",
    "        ##step 1 : get random sessions \n",
    "        s1, seg1,seq_label = self.get_session_flow(item)\n",
    "\n",
    "        ## step 2: replace random word in sentence \n",
    "        s1_random, s1_label, s1_idx = self.random_word(s1)\n",
    "        \n",
    "        segment_label = seg1\n",
    "\n",
    "        netformer_input = s1_random\n",
    "        netformer_label = s1_label\n",
    "        netformer_idx = s1_idx\n",
    "\n",
    "        output = {\"netformer_input\": netformer_input,\n",
    "                  \"netformer_label\": netformer_label,\n",
    "                  \"netformer_idx\":netformer_idx,\n",
    "                  \"segment_label\": segment_label,\n",
    "                \"sequence_label\": seq_label}\n",
    "\n",
    "        return {key: torch.tensor(value,dtype=torch.float32) for key, value in output.items()}\n",
    "\n",
    "\n",
    "    def random_word(self, sentence):\n",
    "        output_label = []\n",
    "        output = []\n",
    "        output_idx =[]\n",
    "        for i, token in enumerate(sentence):\n",
    "            prob = random.random()\n",
    "\n",
    "            if prob < self.mask_ratio:\n",
    "                prob /= self.mask_ratio\n",
    "    \n",
    "                if prob < 0.8:\n",
    "                    output.append(self.special_token_dict['MASK'])\n",
    "                elif prob < 0.9:\n",
    "                    output.append(self.random_selection(self.sessions))\n",
    "                else:\n",
    "                    output.append(token)\n",
    "    \n",
    "                output_label.append(token)\n",
    "                output_idx.append(1)\n",
    "    \n",
    "            else:\n",
    "                output.append(token)\n",
    "                output_label.append(0)\n",
    "                output_idx.append(0)\n",
    "\n",
    "        assert len(output) == len(output_label)\n",
    "        return output, output_label, output_idx\n",
    "        \n",
    "\n",
    "    def random_selection(self, input_sequences):\n",
    "        rand_session = random.randrange(len(input_sequences))\n",
    "        rand_flow = random.randrange(len(input_sequences[rand_session]))\n",
    "        return input_sequences[rand_session][rand_flow]\n",
    "        \n",
    "\n",
    "    def get_session_flow(self, item):\n",
    "        '''Return session data and segments'''\n",
    "        return self.sessions[item], self.segments[item],self.labels[item]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "0e8b310d-731d-42b5-9fe3-ade922daada6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_the_final_input_data(input_sequences,input_segments, input_labels):\n",
    "    # Desired length for each piece\n",
    "    MAX_LEN = 1000\n",
    "    # Padding value (you can use any value you prefer)\n",
    "    padding_value = 0\n",
    "\n",
    "    special_tokens = ['PAD', 'MASK']\n",
    "    special_token_dict = {}\n",
    "    for i in range(len(special_tokens)):\n",
    "        special_token_dict[special_tokens[i]] = 1026+i+1\n",
    "    \n",
    "    # print(special_token_dict)\n",
    "\n",
    "    train_data = NetformerDatasetDownstream(input_sequences,input_segments, input_labels,seq_len=MAX_LEN)\n",
    "    return train_data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "949f66c2-90fa-4f99-aa71-e0e8b913606d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labeling function for the given dataset\n",
    "def label_data(base_path, attack_labels):\n",
    "    file_list = []\n",
    "    for folder in attack_labels:\n",
    "        attack_file = f\"{base_path}/{folder}/{folder}-attack.csv\"\n",
    "        benign_file = f\"{base_path}/{folder}/{folder}-benign.csv\"\n",
    "        \n",
    "        # Read attack files and label them\n",
    "        df_attack = pd.read_csv(attack_file)\n",
    "        df_attack['label'] = attack_labels[folder]\n",
    "        file_list.append((attack_file, attack_labels[folder]))\n",
    "        \n",
    "        # Read benign files and label them\n",
    "        df_benign = pd.read_csv(benign_file)\n",
    "        df_benign['label'] = 0\n",
    "        file_list.append((benign_file, 0))\n",
    "        \n",
    "    return file_list\n",
    "\n",
    "# Define attack labels\n",
    "attack_labels = {\n",
    "    'Botnet': 1,\n",
    "    'BruteForce': 2,\n",
    "    'DDoS': 3,\n",
    "    'DoS': 4,\n",
    "    'Infiltration': 5\n",
    "}\n",
    "\n",
    "# Base path of your dataset\n",
    "base_path = '/home/binghui/NDSS2025/Intrution-detection/CICdataset2018'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "5943b2c0-db76-424a-90a2-d87be15cc6be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: /home/binghui/NDSS2025/Intrution-detection/CICdataset2018/Infiltration/Infiltration-attack.csv with label: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing sessions: 100%|█████████████████████| 1/1 [00:00<00:00, 33825.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing sessions: 100%|█████████████████████| 1/1 [00:00<00:00, 13231.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Processing function, refined and completed\n",
    "print(f\"Processing file: {file_name} with label: {label}\")\n",
    "    # df_packet_token, df_flow_token = processing_file(file_name)\n",
    "input_sequences, input_segments, input_labels = input_generation(df_flow_token, df_packet_token, label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "8e1d5054-ceb1-4d93-8172-d837bb9e830c",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Save the processed data to individual files\n",
    "with open(output_file, 'wb') as f:\n",
    "        pickle.dump((input_sequences, input_segments, input_labels), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d82244a-6094-4fe0-a0d8-ca02177e0abb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d31e1f10-8ab6-4050-a399-0cdb9ff668a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f0983ca-9fa6-42c5-92c3-d2dfb1d40c4a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "065f71e7-d905-47ff-aab9-a40132f79d86",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "011659ca-296b-423c-849d-411586bba1bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "19c463a2-7b49-4dc9-83d0-f1ed97a9e6c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name_list = [('/home/binghui/NDSS2025/Intrution-detection/CICdataset2018/Botnet/Botnet-attack.csv', 1), ('/home/binghui/NDSS2025/Intrution-detection/CICdataset2018/Botnet/Botnet-benign.csv', 0), ('/home/binghui/NDSS2025/Intrution-detection/CICdataset2018/BruteForce/BruteForce-attack.csv', 2), ('/home/binghui/NDSS2025/Intrution-detection/CICdataset2018/BruteForce/BruteForce-benign.csv', 0), ('/home/binghui/NDSS2025/Intrution-detection/CICdataset2018/DDoS/DDoS-attack.csv', 3), ('/home/binghui/NDSS2025/Intrution-detection/CICdataset2018/DDoS/DDoS-benign.csv', 0), ('/home/binghui/NDSS2025/Intrution-detection/CICdataset2018/DoS/DoS-attack.csv', 4), ('/home/binghui/NDSS2025/Intrution-detection/CICdataset2018/DoS/DoS-benign.csv', 0), ('/home/binghui/NDSS2025/Intrution-detection/CICdataset2018/Infiltration/Infiltration-attack.csv', 5), ('/home/binghui/NDSS2025/Intrution-detection/CICdataset2018/Infiltration/Infiltration-benign.csv', 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5aae3ccb-2ea5-4e4d-9c7a-a651699d6dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_benign_name_list = [('/home/binghui/NDSS2025/Intrution-detection/CICdataset2018/Botnet/Botnet-benign.csv', 0), ('/home/binghui/NDSS2025/Intrution-detection/CICdataset2018/BruteForce/BruteForce-benign.csv', 0), ('/home/binghui/NDSS2025/Intrution-detection/CICdataset2018/DDoS/DDoS-benign.csv', 0),  ('/home/binghui/NDSS2025/Intrution-detection/CICdataset2018/DoS/DoS-benign.csv', 0), ('/home/binghui/NDSS2025/Intrution-detection/CICdataset2018/Infiltration/Infiltration-benign.csv', 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c2300681-0183-4462-a314-88b0a71a0633",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_attack_name_list = [('/home/binghui/NDSS2025/Intrution-detection/CICdataset2018/Botnet/Botnet-attack.csv', 1), ('/home/binghui/NDSS2025/Intrution-detection/CICdataset2018/BruteForce/BruteForce-attack.csv', 2),  ('/home/binghui/NDSS2025/Intrution-detection/CICdataset2018/DDoS/DDoS-attack.csv', 3), ('/home/binghui/NDSS2025/Intrution-detection/CICdataset2018/DoS/DoS-attack.csv', 4), ('/home/binghui/NDSS2025/Intrution-detection/CICdataset2018/Infiltration/Infiltration-attack.csv', 5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "717b6546-b635-46fd-96ab-5ede1dcf1ec4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420cf808-8d69-4f45-93c9-19e2ae5e3573",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b4dcd4-88b4-462a-848c-1d53f27a8db4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a2cf008-5001-4a6d-bb79-dc404c1359ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "27f76427-5ded-4eff-a3b0-f4efbd552dd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/home/binghui/NDSS2025/Anomaly-detection/CICdataset2018/output-1000/Botnet-attack.pkl', '/home/binghui/NDSS2025/Anomaly-detection/CICdataset2018/output-1000/BruteForce-attack.pkl', '/home/binghui/NDSS2025/Anomaly-detection/CICdataset2018/output-1000/DDoS-attack.pkl', '/home/binghui/NDSS2025/Anomaly-detection/CICdataset2018/output-1000/DoS-attack.pkl', '/home/binghui/NDSS2025/Anomaly-detection/CICdataset2018/output-1000/Infiltration-attack.pkl']\n"
     ]
    }
   ],
   "source": [
    "final_output_file = '/home/binghui/NDSS2025/Intrution-detection/CICdataset2018/CIC2018-dataset-all-attack-1000.pkl'\n",
    "output_path = '/home/binghui/NDSS2025/Anomaly-detection/CICdataset2018/output-1000'\n",
    "processed_files = []\n",
    "for file_name, label in file_attack_name_list:\n",
    "    output_file = os.path.join(output_path, os.path.basename(file_name).replace('.csv', '.pkl'))\n",
    "    processed_files.append(output_file)\n",
    "print(processed_files)\n",
    "# Combining function to concatenate saved data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "8ea8b565-c32c-4b3d-99d1-b863ec51944d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_saved_data(file_paths, output_file):\n",
    "    all_input_sequences, all_input_segments, all_input_labels = [], [], []\n",
    "\n",
    "    for file_path in file_paths:\n",
    "        with open(file_path, 'rb') as f:\n",
    "            input_sequences, input_segments, input_labels = pickle.load(f)\n",
    "            if len(all_input_sequences) == 0:\n",
    "                all_input_sequences = input_sequences\n",
    "                all_input_segments = input_segments\n",
    "                all_input_labels = input_labels\n",
    "            else:\n",
    "                all_input_sequences = np.concatenate((all_input_sequences, input_sequences), axis=0)\n",
    "                all_input_segments = np.concatenate((all_input_segments, input_segments), axis=0)\n",
    "                all_input_labels = np.concatenate((all_input_labels, input_labels), axis=0)\n",
    "    print(np.unique(all_input_labels))\n",
    "    all_input_labels = np.where(all_input_labels == 0, 0, 1)\n",
    "    print(np.unique(all_input_labels))\n",
    "    train_data = NetformerDatasetDownstream(all_input_sequences, all_input_segments, all_input_labels, seq_len=200)\n",
    "    with open(output_file, 'wb') as f:\n",
    "        pickle.dump(train_data, f)\n",
    "\n",
    "    return output_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "97c9cee6-1c51-457b-a504-072aade58109",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 2. 3. 4. 5.]\n",
      "[1]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/home/binghui/NDSS2025/Intrution-detection/CICdataset2018/CIC2018-dataset-all-attack-1000.pkl'"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combine_saved_data(processed_files, final_output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b90271-dbcb-4f44-9b22-a61fe0e4bcaf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
