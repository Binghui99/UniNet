{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f44129cc-2f1f-4b07-92e9-d6a57d880b84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Jul  2 13:49:30 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.183.01             Driver Version: 535.183.01   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA GeForce RTX 4080        Off | 00000000:01:00.0  On |                  N/A |\n",
      "|  0%   38C    P8               7W / 320W |   4548MiB / 16376MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A      1865      G   /usr/lib/xorg/Xorg                          124MiB |\n",
      "|    0   N/A  N/A      2065      G   /usr/bin/gnome-shell                         67MiB |\n",
      "|    0   N/A  N/A      3871      G   ...seed-version=20240701-180127.641000      145MiB |\n",
      "|    0   N/A  N/A      4115      C   ...da3/envs/backdoor-attack/bin/python     3954MiB |\n",
      "|    0   N/A  N/A      5948      C   ...da3/envs/backdoor-attack/bin/python      238MiB |\n",
      "+---------------------------------------------------------------------------------------+\n",
      "Using device:  cuda (NVIDIA GeForce RTX 4080)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import Tensor\n",
    "from torchsummary import summary\n",
    "import random\n",
    "from numpy import load\n",
    "from tqdm import tqdm, trange\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pickle\n",
    "import os\n",
    "from collections import Counter\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score, roc_auc_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "%run './Attention_based_model.ipynb'\n",
    "# check the availability of cuda\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device: \", device, f\"({torch.cuda.get_device_name(device)})\" if torch.cuda.is_available() else \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "33fdaaf4-7ec7-4311-a802-57293f68b7d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./new_dataset/CIC2018-dataset-all-benign-1000.pkl', 'rb') as f:\n",
    "    test_data = pickle.load(f)  \n",
    "test_loader = DataLoader(test_data, batch_size=32, shuffle=True, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "58f92aac-fdce-465c-931a-26e822a5c2cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 961/961 [00:01<00:00, 547.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data points shape: (30750, 200)\n",
      "Labels shape: (30750,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "data_points = []\n",
    "labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    # progress bar\n",
    "    data_iter = tqdm.tqdm(\n",
    "        enumerate(test_loader),\n",
    "        total=len(test_loader),\n",
    "        bar_format=\"{l_bar}{r_bar}\"\n",
    "    )\n",
    "    for i, data in data_iter:  # Assuming you have an inference data loader\n",
    "        data = {key: value.to(device) for key, value in data.items()}\n",
    "        inputs, label = data[\"netformer_input\"], data[\"sequence_label\"]\n",
    "        \n",
    "        inputs = inputs.cpu().numpy()\n",
    "        label = label.cpu().numpy()\n",
    "        \n",
    "        # Collect the data points and labels\n",
    "        for j in range(inputs.shape[0]):\n",
    "            data_points.append(inputs[j])\n",
    "            labels.append(label[j])\n",
    "\n",
    "# Convert lists to numpy arrays\n",
    "data_points_np = np.array(data_points)\n",
    "labels_np = np.array(labels)\n",
    "\n",
    "# Optionally save to files\n",
    "np.save('data_for_baselines_benign.npy', data_points_np)\n",
    "np.save('labels_for_baselines_benign.npy', labels_np)\n",
    "\n",
    "# Print shapes to verify\n",
    "print('Data points shape:', data_points_np.shape)\n",
    "print('Labels shape:', labels_np.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "68c5690e-c460-4082-866b-8e492c18de39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Training\n",
      "(7565, 1000, 10)\n",
      "(7565,)\n",
      "Counter({0.0: 7565})\n",
      "testing\n",
      "(10000, 1000, 10)\n",
      "(10000,)\n",
      "Counter({0.0: 5000, 1.0: 5000})\n",
      "Reshaping data if necessary...\n"
     ]
    }
   ],
   "source": [
    "## Load the training and test datasets\n",
    "print(\"Loading data...\")\n",
    "train_data_points = np.load('data_points_benign_training.npy')\n",
    "train_labels = np.load('labels_benign_training.npy')\n",
    "test_data_points = np.load('data_points_testing.npy')\n",
    "test_labels = np.load('labels_testing.npy')\n",
    "\n",
    "print('Training')\n",
    "print(train_data_points.shape)\n",
    "print(train_labels.shape)\n",
    "print(Counter(train_labels))\n",
    "\n",
    "print('testing')\n",
    "print(test_data_points.shape)\n",
    "print(test_labels.shape)\n",
    "print(Counter(test_labels))\n",
    "\n",
    "\n",
    "# Verify and reshape the data if necessary\n",
    "print(\"Reshaping data if necessary...\")\n",
    "if len(train_data_points.shape) == 3:\n",
    "    train_data_points = train_data_points.reshape(train_data_points.shape[0], -1)  # Flatten to (n_samples, n_features)\n",
    "if len(test_data_points.shape) == 3:\n",
    "    test_data_points = test_data_points.reshape(test_data_points.shape[0], -1)  # Flatten to (n_samples, n_features)\n",
    "\n",
    "# Scale the data\n",
    "scaler = MinMaxScaler()\n",
    "train_data_points_scaled = scaler.fit_transform(train_data_points)\n",
    "test_data_points_scaled = scaler.transform(test_data_points)\n",
    "\n",
    "# Convert labels to binary (1 for anomaly, -1 for normal)\n",
    "train_labels_binary = np.where(train_labels == 1, 1, -1)\n",
    "test_labels_binary = np.where(test_labels == 1, 1, -1)\n",
    "\n",
    "# Initialize results dataframe\n",
    "results = pd.DataFrame(columns=['Model', 'F1 Score', 'Accuracy', 'Precision', 'Recall', 'AUC'])\n",
    "\n",
    "# Prepare data for PyTorch\n",
    "train_data_tensor = torch.tensor(train_data_points_scaled, dtype=torch.float32)\n",
    "test_data_tensor = torch.tensor(test_data_points_scaled, dtype=torch.float32)\n",
    "train_labels_tensor = torch.tensor(train_labels_binary, dtype=torch.float32)\n",
    "test_labels_tensor = torch.tensor(test_labels_binary, dtype=torch.float32)\n",
    "train_dataset = TensorDataset(train_data_tensor, train_labels_tensor)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "48ee4530-fe45-46f8-bd6c-2cb4454779a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Autoencoder Model\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_dim, encoding_dim):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, encoding_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(encoding_dim, input_dim),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "\n",
    "# Variational Autoencoder Model\n",
    "class VariationalAutoencoder(nn.Module):\n",
    "    def __init__(self, input_dim, latent_dim):\n",
    "        super(VariationalAutoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, 64),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.z_mean = nn.Linear(64, latent_dim)\n",
    "        self.z_log_var = nn.Linear(64, latent_dim)\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, input_dim),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h = self.encoder(x)\n",
    "        z_mean = self.z_mean(h)\n",
    "        z_log_var = self.z_log_var(h)\n",
    "        std = torch.exp(0.5 * z_log_var)\n",
    "        eps = torch.randn_like(std)\n",
    "        z = z_mean + eps * std\n",
    "        x_decoded = self.decoder(z)\n",
    "        return x_decoded, z_mean, z_log_var\n",
    "\n",
    "# LSTM Autoencoder Model\n",
    "class LSTMAutoencoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers):\n",
    "        super(LSTMAutoencoder, self).__init__()\n",
    "        self.encoder = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.decoder = nn.LSTM(hidden_dim, input_dim, num_layers, batch_first=True)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        _, (hidden, _) = self.encoder(x)\n",
    "        decoded, _ = self.decoder(hidden.repeat(x.size(1), 1, 1).permute(1, 0, 2))\n",
    "        return decoded\n",
    "\n",
    "# Function to train and evaluate the models\n",
    "def train_model(model, dataloader, num_epochs=50):\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        for data, _ in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, data)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "def evaluate_model(model, data):\n",
    "    with torch.no_grad():\n",
    "        reconstructions = model(data).numpy()\n",
    "        mse_loss = np.mean(np.power(data.numpy() - reconstructions, 2), axis=1)\n",
    "    return mse_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c0bc48ad-d585-4d13-a942-1b3e8f342f5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training LSTM Autoencoder...\n"
     ]
    }
   ],
   "source": [
    "# Training LSTM Autoencoder\n",
    "print(\"Training LSTM Autoencoder...\")\n",
    "timesteps = 10\n",
    "train_data_points_reshaped = train_data_points_scaled.reshape((train_data_points_scaled.shape[0], timesteps, train_data_points_scaled.shape[1] // timesteps))\n",
    "test_data_points_reshaped = test_data_points_scaled.reshape((test_data_points_scaled.shape[0], timesteps, test_data_points_scaled.shape[1] // timesteps))\n",
    "train_data_tensor_reshaped = torch.tensor(train_data_points_reshaped, dtype=torch.float32)\n",
    "test_data_tensor_reshaped = torch.tensor(test_data_points_reshaped, dtype=torch.float32)\n",
    "train_dataset_reshaped = TensorDataset(train_data_tensor_reshaped, train_labels_tensor)\n",
    "train_dataloader_reshaped = DataLoader(train_dataset_reshaped, batch_size=32, shuffle=True)\n",
    "\n",
    "input_dim = train_data_points_scaled.shape[1] // timesteps\n",
    "hidden_dim = 100\n",
    "num_layers = 1\n",
    "lstm_ae = LSTMAutoencoder(input_dim, hidden_dim, num_layers)\n",
    "train_model(lstm_ae, train_dataloader_reshaped)\n",
    "\n",
    "# Evaluating LSTM Autoencoder\n",
    "with torch.no_grad():\n",
    "    reconstructions = lstm_ae(test_data_tensor_reshaped).numpy()\n",
    "    mse_loss = np.mean(np.power(test_data_tensor_reshaped.numpy() - reconstructions, 2), axis=(1, 2))\n",
    "threshold = np.percentile(mse_loss, 95)\n",
    "lstm_ae_pred = (mse_loss > threshold).astype(int)\n",
    "lstm_ae_pred = np.where(lstm_ae_pred == 1, 1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a4b9096e-01d1-4fde-8a29-2e2bbc4f3833",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate performance metrics\n",
    "def calculate_metrics(y_true, y_pred, model_name):\n",
    "    print(f\"Calculating metrics for {model_name}...\")\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred)\n",
    "    recall = recall_score(y_true, y_pred)\n",
    "    auc = roc_auc_score(y_true, y_pred)\n",
    "    return pd.Series([model_name, f1, accuracy, precision, recall, auc], \n",
    "                     index=['Model', 'F1 Score', 'Accuracy', 'Precision', 'Recall', 'AUC'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d597674f-83b9-4e7f-a913-553f8cc0fcbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating metrics for LSTM Autoencoder...\n",
      "Displaying results...\n",
      "              Model F1 Score Accuracy Precision Recall   AUC\n",
      "0  LSTM Autoencoder      0.0     0.45       0.0    0.0  0.45\n",
      "Saving results to CSV file...\n",
      "Script execution completed.\n"
     ]
    }
   ],
   "source": [
    "results = pd.concat([results, calculate_metrics(test_labels_binary, lstm_ae_pred, 'LSTM Autoencoder').to_frame().T], ignore_index=True)\n",
    "\n",
    "# Display results\n",
    "print(\"Displaying results...\")\n",
    "print(results)\n",
    "\n",
    "# Save results to a CSV file\n",
    "print(\"Saving results to CSV file...\")\n",
    "results.to_csv('deep_learning_anomaly_detection_baseline_results.csv', index=False)\n",
    "\n",
    "print(\"Script execution completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69db2153-140e-4e08-a3eb-a0cb12c48787",
   "metadata": {},
   "outputs": [],
   "source": [
    "              Model  F1 Score Accuracy Precision  Recall     AUC\n",
    "0       Autoencoder  0.077818   0.4928     0.428  0.0428  0.4928\n",
    "1  LSTM Autoencoder  0.152727    0.534      0.84   0.084   0.534\n",
    "              Model F1 Score Accuracy Precision Recall   AUC\n",
    "0  LSTM Autoencoder      0.0     0.45       0.0    0.0  0.45\n",
    "Saving results to CSV file...\n",
    "Script execution completed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aa1159f0-48c3-4a8d-95a0-46682464344b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Autoencoder...\n",
      "Evaluating Autoencoder...\n"
     ]
    }
   ],
   "source": [
    "# Training Autoencoder\n",
    "print(\"Training Autoencoder...\")\n",
    "input_dim = train_data_points_scaled.shape[1]\n",
    "encoding_dim = 32\n",
    "ae = Autoencoder(input_dim, encoding_dim)\n",
    "train_model(ae, train_dataloader)\n",
    "print(\"Evaluating Autoencoder...\")\n",
    "# Evaluating Autoencoder\n",
    "mse_loss = evaluate_model(ae, test_data_tensor)\n",
    "threshold = np.percentile(mse_loss, 95)\n",
    "ae_pred = (mse_loss > threshold).astype(int)\n",
    "ae_pred = np.where(ae_pred == 1, 1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5d67fd16-1217-4265-8c0e-57cb7303104a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating and storing results...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'labels_binary' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCalculating and storing results...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m results \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([results, calculate_metrics(labels_binary, ae_pred, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAutoencoder\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mto_frame()\u001b[38;5;241m.\u001b[39mT], ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(results)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'labels_binary' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"Calculating and storing results...\")\n",
    "results = pd.concat([results, calculate_metrics(labels_binary, ae_pred, 'Autoencoder').to_frame().T], ignore_index=True)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c67134e3-407f-4a3f-a459-8acae5babb17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Variational Autoencoder...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'input_dim' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 25\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining Variational Autoencoder...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     24\u001b[0m latent_dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[0;32m---> 25\u001b[0m vae \u001b[38;5;241m=\u001b[39m VariationalAutoencoder(input_dim, latent_dim)\n\u001b[1;32m     26\u001b[0m train_vae_model(vae, train_dataloader)\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEvaluating Variational Autoencoder...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'input_dim' is not defined"
     ]
    }
   ],
   "source": [
    "# Function to train and evaluate the Variational Autoencoder model\n",
    "def train_vae_model(model, dataloader, num_epochs=50):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        for data, _ in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            x_decoded, z_mean, z_log_var = model(data)\n",
    "            recon_loss = nn.functional.mse_loss(x_decoded, data, reduction='sum')\n",
    "            kl_loss = -0.5 * torch.sum(1 + z_log_var - z_mean**2 - torch.exp(z_log_var))\n",
    "            loss = recon_loss + kl_loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "def evaluate_vae_model(model, data):\n",
    "    with torch.no_grad():\n",
    "        reconstructions, _, _ = model(data)\n",
    "        mse_loss = np.mean(np.power(data.numpy() - reconstructions.numpy(), 2), axis=1)\n",
    "    return mse_loss\n",
    "\n",
    "\n",
    "# Training Variational Autoencoder\n",
    "print(\"Training Variational Autoencoder...\")\n",
    "latent_dim = 2\n",
    "vae = VariationalAutoencoder(input_dim, latent_dim)\n",
    "train_vae_model(vae, train_dataloader)\n",
    "print(\"Evaluating Variational Autoencoder...\")\n",
    "# Evaluating Variational Autoencoder\n",
    "mse_loss = evaluate_vae_model(vae, test_data_tensor)\n",
    "threshold = np.percentile(mse_loss, 95)\n",
    "vae_pred = (mse_loss > threshold).astype(int)\n",
    "vae_pred = np.where(vae_pred == 1, 1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ab1440-1de7-4493-a3f0-f8e3053eb083",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
