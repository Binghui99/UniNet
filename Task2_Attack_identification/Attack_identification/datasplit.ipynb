{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8037ae94-4770-4e77-ac48-4959ccb5703f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "# Define the NetformerDatasetDownstream class\n",
    "class NetformerDatasetDownstream(Dataset):\n",
    "    def __init__(self, input_sequences, input_labels, input_segments, seq_len=2000):\n",
    "        self.seq_len = seq_len\n",
    "        self.session_flows = len(input_sequences)\n",
    "        self.sessions = input_sequences\n",
    "        self.segments = input_segments\n",
    "        self.labels = input_labels\n",
    "        self.special_token_dict = {'PAD': 0, 'MASK': 1028}\n",
    "        self.mask_ratio = 0\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.session_flows\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        s1, seg1, seq_label = self.get_session_flow(item)\n",
    "        s1_random, s1_label, s1_idx = self.random_word(s1)\n",
    "        segment_label = seg1\n",
    "        netformer_input = s1_random\n",
    "        netformer_label = s1_label\n",
    "        netformer_idx = s1_idx\n",
    "\n",
    "        output = {\n",
    "            \"netformer_input\": netformer_input,\n",
    "            \"netformer_label\": netformer_label,\n",
    "            \"netformer_idx\": netformer_idx,\n",
    "            \"segment_label\": segment_label,\n",
    "            \"sequence_label\": seq_label\n",
    "        }\n",
    "\n",
    "        return {key: torch.tensor(value, dtype=torch.float32) for key, value in output.items()}\n",
    "\n",
    "    def random_word(self, sentence):\n",
    "        output_label = []\n",
    "        output = []\n",
    "        output_idx = []\n",
    "\n",
    "        for i, token in enumerate(sentence):\n",
    "            prob = random.random()\n",
    "            if prob < self.mask_ratio:\n",
    "                prob /= self.mask_ratio\n",
    "                if prob < 0.8:\n",
    "                    output.append(self.special_token_dict['MASK'])\n",
    "                elif prob < 0.9:\n",
    "                    output.append(self.random_selection(self.sessions))\n",
    "                else:\n",
    "                    output.append(token)\n",
    "                output_label.append(token)\n",
    "                output_idx.append(1)\n",
    "            else:\n",
    "                output.append(token)\n",
    "                output_label.append(0)\n",
    "                output_idx.append(0)\n",
    "\n",
    "        assert len(output) == len(output_label)\n",
    "        return output, output_label, output_idx\n",
    "\n",
    "    def random_selection(self, input_sequences):\n",
    "        rand_session = random.randrange(len(input_sequences))\n",
    "        rand_flow = random.randrange(len(input_sequences[rand_session]))\n",
    "        return input_sequences[rand_session][rand_flow]\n",
    "\n",
    "    def get_session_flow(self, item):\n",
    "        return self.sessions[item], self.segments[item], self.labels[item]\n",
    "\n",
    "# Load the PKL dataset\n",
    "with open('CIC2018-dataset-all-new.pkl', 'rb') as f:\n",
    "    netformer_dataset = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6917cbf1-792f-49d1-b39d-870a473fe268",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to sample the dataset\n",
    "def sample_dataset(dataset, target_labels, samples_per_label):\n",
    "    label_indices = {label: [] for label in target_labels}\n",
    "    \n",
    "    # Collect indices for each target label\n",
    "    for idx in range(len(dataset)):\n",
    "        label = int(dataset.labels[idx])\n",
    "        if label in target_labels:\n",
    "            label_indices[label].append(idx)\n",
    "    \n",
    "    sampled_indices = []\n",
    "    for label in target_labels:\n",
    "        if len(label_indices[label]) < samples_per_label:\n",
    "            raise ValueError(f\"Not enough samples for label {label}\")\n",
    "        sampled_indices.extend(random.sample(label_indices[label], samples_per_label))\n",
    "    \n",
    "    # Create a subset of the dataset\n",
    "    return Subset(dataset, sampled_indices)\n",
    "\n",
    "# Specify the target labels and samples per label\n",
    "target_labels = [0, 1, 2, 4]\n",
    "samples_per_label = 200\n",
    "\n",
    "# Sample the dataset\n",
    "sampled_dataset = sample_dataset(netformer_dataset, target_labels, samples_per_label)\n",
    "# Relabel the dataset after sampling\n",
    "def relabel_dataset(dataset, label_mapping):\n",
    "    for idx in dataset.indices:\n",
    "        item = dataset.dataset[idx]\n",
    "        original_label = int(item['sequence_label'].item())\n",
    "        new_label = label_mapping[original_label]\n",
    "        item['sequence_label'] = torch.tensor(new_label, dtype=torch.float32)\n",
    "\n",
    "label_mapping = {0: 0, 1: 1, 2: 2, 4: 3}\n",
    "relabel_dataset(sampled_dataset, label_mapping)\n",
    "\n",
    "# Function to convert subset to required format and save\n",
    "def save_dataset_as_netformer_format(dataset, filename):\n",
    "    output = {\n",
    "        'netformer_input': [],\n",
    "        'netformer_label': [],\n",
    "        'netformer_idx': [],\n",
    "        'segment_label': [],\n",
    "        'sequence_label': []\n",
    "    }\n",
    "    \n",
    "    for idx in dataset.indices:\n",
    "        item = dataset.dataset[idx]\n",
    "        output['netformer_input'].append(item['netformer_input'].tolist())\n",
    "        output['netformer_label'].append(item['netformer_label'].tolist())\n",
    "        output['netformer_idx'].append(item['netformer_idx'].tolist())\n",
    "        output['segment_label'].append(item['segment_label'].tolist())\n",
    "        output['sequence_label'].append(int(item['sequence_label'].item()))\n",
    "    \n",
    "    with open(filename, 'wb') as f:\n",
    "        pickle.dump(output, f)\n",
    "\n",
    "# Save the sampled and relabeled dataset into a PKL file\n",
    "save_dataset_as_netformer_format(sampled_dataset, 'CIC2018-dataset-sampled-200.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea77798-da85-47fa-911b-4c3759ee3974",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
