{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb5e4a61-ce01-4875-a310-279ff9c1daf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import dask.dataframe as dd\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import torch\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92ddc12f-2eeb-4f3c-a668-35a17baeb69d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_name = \"/home/binghui/NDSS2025/Intrution-detection/CICdataset2018/BotNet/Botnet-attack.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4375d3a6-6113-4690-a7e2-d987abc38b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "## help functions\n",
    "# Define a function to modify 'ServePort' based on the condition\n",
    "def modify_serve_port(df):\n",
    "    df['ServePort'] = np.where(df['ServePort'] > 1024, 1025, df['ServePort'])\n",
    "    return df\n",
    "\n",
    "def assign_flow_id(df):\n",
    "    # Function to create a sorted string from IPs and Ports\n",
    "    def create_flow_identifier(row):\n",
    "        # Sort IPs\n",
    "        ips = sorted([row['IP_source'], row['IP_destination']])\n",
    "        # Sort Ports\n",
    "        ports = sorted([row['Port_source'], row['Port_destination']])\n",
    "        # Combine with Layer 4 protocol\n",
    "        return f\"{ips[0]}_{ips[1]}_{ports[0]}_{ports[1]}_{row['Layer_4_protocol']}\"\n",
    "\n",
    "    # Apply the function to each row to create a flow identifier\n",
    "    df['flow_identifier'] = df.apply(create_flow_identifier, axis=1)\n",
    "\n",
    "    # Assign flow ID based on the unique flow identifiers\n",
    "    df['flow_id'] = pd.factorize(df['flow_identifier'])[0]\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def create_flow_summary(df):\n",
    "    # Group by flow_id and aggregate\n",
    "    df['serve_ip'] = df.apply(lambda x: min(x['IP_source'], x['IP_destination']), axis=1)\n",
    "\n",
    "    flow_summary = df.groupby('flow_id').agg(\n",
    "        serve_ip=pd.NamedAgg(column='serve_ip', aggfunc='first'),  # Include serve_ip in the summary\n",
    "        start_time=pd.NamedAgg(column='TIME', aggfunc='min'),\n",
    "        end_time=pd.NamedAgg(column='TIME', aggfunc='max'),\n",
    "        average_packet_size=pd.NamedAgg(column='Size', aggfunc='mean'),\n",
    "        average_IAT=pd.NamedAgg(column='IAT', aggfunc='mean'),\n",
    "        num_packets=pd.NamedAgg(column='TIME', aggfunc='count'),\n",
    "        l4_protocol = pd.NamedAgg(column='Layer_4_protocol', aggfunc='first'),\n",
    "        srcIP=pd.NamedAgg(column='IP_source', aggfunc='first'),  # Extract the Source IP\n",
    "        dstIP=pd.NamedAgg(column='IP_destination', aggfunc='first'),  # Extract the Destination IP\n",
    "        ServePort = pd.NamedAgg(column='ServePort', aggfunc='first')\n",
    "    )\n",
    "\n",
    "    # Calculate duration\n",
    "    flow_summary['duration'] = flow_summary['end_time'] - flow_summary['start_time']\n",
    "    # Determine if flow is bidirectional\n",
    "    flow_summary['direction'] = df.groupby('flow_id').apply(\n",
    "        lambda x: 1 if len(x['IP_source'].unique()) > 1 else 0\n",
    "    )\n",
    "    flow_summary = flow_summary.reset_index()\n",
    "    return flow_summary\n",
    "\n",
    "last_end_time = {}\n",
    "global_session_id = 0\n",
    "# Function to assign session IDs\n",
    "def assign_session_id(row):\n",
    "    global global_session_id\n",
    "\n",
    "    serve_ip = row['serve_ip']\n",
    "    start_time = row['start_time']\n",
    "    \n",
    "    # Initialize if this is the first flow for the serve_ip\n",
    "    if serve_ip not in last_end_time:\n",
    "        last_end_time[serve_ip] = row['end_time']\n",
    "        global_session_id += 1\n",
    "        return global_session_id\n",
    "    \n",
    "    # Calculate inter-time\n",
    "    inter_time = (start_time - last_end_time[serve_ip]).total_seconds()\n",
    "    \n",
    "    # Check if new session should start\n",
    "    if inter_time > 30:\n",
    "        global_session_id += 1\n",
    "    \n",
    "    # Update the last end time\n",
    "    last_end_time[serve_ip] = row['end_time']\n",
    "    return global_session_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3dc52834-7784-4910-8797-bc05d4f7ce53",
   "metadata": {},
   "outputs": [],
   "source": [
    "## help functions\n",
    "### Tokenization Binning\n",
    "# process for the Packet\n",
    "def equal_width_binning_packet(df):\n",
    "    n_bins = 1026\n",
    "    strategy = 'uniform' # quantile for equal-frequency, kmeans for k-clustering\n",
    "    subsample_size = 200000  # Set this to None to disable subsampling\n",
    "    scalers = {\n",
    "        'IAT': KBinsDiscretizer(n_bins=n_bins, encode='ordinal', strategy=strategy, subsample=subsample_size),\n",
    "        'Size': KBinsDiscretizer(n_bins=n_bins, encode='ordinal', strategy=strategy, subsample=subsample_size),\n",
    "        'Payload_Size': KBinsDiscretizer(n_bins=n_bins, encode='ordinal', strategy=strategy, subsample=subsample_size)  \n",
    "    }\n",
    "\n",
    "    df_copy = df.copy()\n",
    "    for feature, scaler in scalers.items():\n",
    "        scaler.fit(df_copy[[feature]])\n",
    "        df_copy[feature] = scaler.transform(df_copy[[feature]]).astype(int)\n",
    "    return df_copy\n",
    "\n",
    "# process for the flow\n",
    "def equal_width_binning_flow(df):\n",
    "    n_bins = 1026\n",
    "    strategy = 'uniform' # quantile for equal-frequency, kmeans for k-clustering\n",
    "    subsample_size = 200000  # Set this to None to disable subsampling\n",
    "    scaler = KBinsDiscretizer(n_bins=n_bins, encode='ordinal', strategy=strategy, subsample=subsample_size)\n",
    "\n",
    "    df_copy = df.copy()\n",
    "    columns_to_bin = ['duration','average_packet_size','average_IAT','num_packets']\n",
    "    \n",
    "    for col in columns_to_bin:\n",
    "        scaler.fit(df_copy[[col]])\n",
    "        df_copy[col] = scaler.transform(df_copy[[col]]).astype(int)\n",
    "    return df_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e6cba95c-a118-4bba-b675-ac7a74890a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def processing_file(file_name):\n",
    "    df = dd.read_csv(file_name)\n",
    "    df['Direction'] = (df['IP_source'] < df['IP_destination']).astype(int)\n",
    "    df['ServePort'] = df[['Port_source', 'Port_destination']].min(axis=1)\n",
    "\n",
    "    # Apply the serve port function\n",
    "    df = df.map_partitions(modify_serve_port, meta=df)\n",
    "    dask_df = df.copy()\n",
    "    dask_df['Flow'] = dask_df['IP_source'] + dask_df['IP_destination'] + dask_df['Port_source'].astype(str) + dask_df['Port_destination'].astype(str) + dask_df['Layer_4_protocol'].astype(str)\n",
    "    dask_df['Inverse_Flow'] = dask_df['IP_destination'] + dask_df['IP_source'] + dask_df['Port_destination'].astype(str) + dask_df['Port_source'].astype(str)  + dask_df['Layer_4_protocol'].astype(str)\n",
    "    ## change from dask df to pandas\n",
    "    df = dask_df.compute()\n",
    "    df = assign_flow_id(df)\n",
    "\n",
    "    \n",
    "    # Sort by flow_id and TIME\n",
    "    df = df.sort_values(by=['flow_id', 'TIME'])\n",
    "    # Calculate Inter-Arrival Time within each flow\n",
    "    df['IAT'] = df.groupby('flow_id')['TIME'].diff().fillna(0)\n",
    "\n",
    "    ## filter the huge size packet\n",
    "    df = df[df['Size'] <= 5000]\n",
    "    df = df[df['Payload_Size']<= 5000]\n",
    "    print(\"Packet Processing Done\")\n",
    "    # Create the flow-level summary DataFrame\n",
    "    flow_summary_df = create_flow_summary(df)\n",
    "\n",
    "    flow_df = flow_summary_df.copy()\n",
    "    print(flow_df.shape)\n",
    "\n",
    "    # Assuming flow_summary_df is your DataFrame\n",
    "    # Convert 'start_time' and 'end_time' to datetime if they are not already\n",
    "    flow_summary_df['start_time'] = pd.to_datetime(flow_summary_df['start_time'])\n",
    "    flow_summary_df['end_time'] = pd.to_datetime(flow_summary_df['end_time'])\n",
    "    \n",
    "    # Sort by serve_ip and start_time\n",
    "    flow_summary_df = flow_summary_df.sort_values(by=['serve_ip', 'start_time'])\n",
    "    \n",
    "    # Initialize a dictionary to track the last end time for each serve_ip\n",
    "    # and a global session ID counter\n",
    "\n",
    "\n",
    "    # Apply function to each row\n",
    "    flow_summary_df['session_id'] = flow_summary_df.apply(assign_session_id, axis=1)\n",
    "\n",
    "\n",
    "    df_with_sessions = pd.merge(df, flow_summary_df[['flow_id', 'session_id']], on='flow_id', how='left')\n",
    "    df_with_sessions = df_with_sessions.sort_values(by=['session_id'])\n",
    "    ## final flow representation \n",
    "    final_flow = flow_summary_df[['session_id','flow_id','direction', 'duration', 'l4_protocol', 'average_packet_size','average_IAT','num_packets','ServePort']].copy()\n",
    "    ## final packet representation\n",
    "    final_packet = df_with_sessions[['session_id','flow_id','Direction', 'Layer_4_protocol','Size','Payload_Size','IAT', 'ServePort']].copy()\n",
    "\n",
    "    print(\"Tokenization: Binning\")\n",
    "    df_packet_token = equal_width_binning_packet(final_packet)\n",
    "    df_flow_token = equal_width_binning_flow(final_flow)\n",
    "\n",
    "    return df_packet_token, df_flow_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "52933b3e-f827-499b-b0ef-77e2ca757159",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_packet_token, df_flow_token = processing_file(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dd1432cf-5e6d-4203-ba46-cab7437685ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "## part 2 input generation \n",
    "def create_embeddings(length, embedding_value):\n",
    "    return np.full(length, embedding_value)\n",
    "\n",
    "def input_generation(df_flow_token, df_packet_token, def_label):\n",
    "    max_len = 2000\n",
    "    padding_value = 0\n",
    "    \n",
    "    session_dict = df_flow_token.groupby('session_id')['flow_id'].apply(list).to_dict()\n",
    "    flow_id_dict = df_packet_token.groupby('flow_id').apply(lambda x: x.drop(['flow_id', 'session_id'], axis=1).values.flatten().tolist()).to_dict()\n",
    "    for key in flow_id_dict.keys():\n",
    "        if len(flow_id_dict[key]) >= 6*9: ## more than 9 packet\n",
    "            flow_id_dict[key] = flow_id_dict[key][:54]  \n",
    "    flow_feature_dict = df_flow_token.groupby('flow_id').apply(lambda x: x.drop(['flow_id', 'session_id'], axis=1).values.flatten().tolist()).to_dict()\n",
    "\n",
    "\n",
    "    # Use the session_dict to create the final dictionary\n",
    "    session_feature_dict = {}\n",
    "    segment_embedding_dict = {}\n",
    "    \n",
    "    # Wrap the outer loop with tqdm for the progress bar\n",
    "    for session_id in tqdm(session_dict.keys(), desc=\"Processing sessions\"):\n",
    "        session_features = []\n",
    "        session_embeddings = []\n",
    "        for flow_id in session_dict[session_id]:\n",
    "            # Retrieve flow features and create flow embeddings\n",
    "            flow_feat = flow_feature_dict[flow_id]\n",
    "            flow_embedding = create_embeddings(len(flow_feat), 0)  # 0 for flow features\n",
    "            session_features.extend(flow_feat)\n",
    "            session_embeddings.extend(flow_embedding)\n",
    "    \n",
    "            # Retrieve packet features and create packet embeddings\n",
    "            packet_feat = flow_id_dict[flow_id]\n",
    "            packet_embedding = create_embeddings(len(packet_feat), 1)  # 1 for packet features\n",
    "            session_features.extend(packet_feat)\n",
    "            session_embeddings.extend(packet_embedding)\n",
    "        \n",
    "        session_feature_dict[session_id] =  session_features\n",
    "        segment_embedding_dict[session_id] = session_embeddings\n",
    "    \n",
    "    print(\"Processing complete.\")\n",
    "\n",
    "\n",
    "\n",
    "    input_sequences = []\n",
    "    input_segments = []\n",
    "    \n",
    "    def pad_sequence(seq, target_len, pad_value=0):\n",
    "        return np.pad(seq, (0, max(target_len - len(seq), 0)), mode='constant', constant_values=pad_value)\n",
    "    \n",
    "    for s_id in tqdm(session_feature_dict.keys(), desc=\"Processing sessions\"):\n",
    "        ids = np.array(session_feature_dict[s_id])  # Convert to numpy array for memory efficiency\n",
    "        segs = np.array(segment_embedding_dict[s_id])  # Convert to numpy array for memory efficiency\n",
    "    \n",
    "        # Splitting and padding the sequences\n",
    "        for i in range(0, len(ids), max_len):\n",
    "            end_idx = min(i + max_len, len(ids))\n",
    "            pieces = pad_sequence(ids[i:end_idx], max_len, padding_value)\n",
    "            seg_pieces = pad_sequence(segs[i:end_idx], max_len, padding_value)\n",
    "    \n",
    "            input_sequences.append(pieces)\n",
    "            input_segments.append(seg_pieces)\n",
    "    \n",
    "    print(\"Processing complete.\")\n",
    "  \n",
    "    labels = def_label * np.ones((len(input_sequences)))\n",
    "\n",
    "    return input_sequences,input_segments,labels \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2bb0d807-12d1-4643-ba9f-6d99a63bd615",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_sequences,input_segments,labels = input_generation(df_flow_token, df_packet_token, 1)\n",
    "# len(input_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "045c823c-ff77-4b87-aac2-65f3b4db3cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetformerDatasetDownstream(Dataset):\n",
    "    def __init__(self, input_sequences, input_segments, input_labels, seq_len = 2000):\n",
    "        self.seq_len = seq_len\n",
    "        self.session_flows = len(input_sequences)\n",
    "        self.sessions = input_sequences\n",
    "        self.segments = input_segments\n",
    "        self.labels = input_labels\n",
    "        self.special_token_dict =  {'PAD': 1027, 'MASK': 1028}\n",
    "        self.mask_ratio = 0\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.session_flows\n",
    "\n",
    "    def __getitem__(self,item):\n",
    "\n",
    "        ##step 1 : get random sessions \n",
    "        s1, seg1,seq_label = self.get_session_flow(item)\n",
    "\n",
    "        ## step 2: replace random word in sentence \n",
    "        s1_random, s1_label, s1_idx = self.random_word(s1)\n",
    "        \n",
    "        segment_label = seg1\n",
    "\n",
    "        netformer_input = s1_random\n",
    "        netformer_label = s1_label\n",
    "        netformer_idx = s1_idx\n",
    "\n",
    "        output = {\"netformer_input\": netformer_input,\n",
    "                  \"netformer_label\": netformer_label,\n",
    "                  \"netformer_idx\":netformer_idx,\n",
    "                  \"segment_label\": segment_label,\n",
    "                \"sequence_label\": seq_label}\n",
    "\n",
    "        return {key: torch.tensor(value,dtype=torch.float32) for key, value in output.items()}\n",
    "\n",
    "\n",
    "    def random_word(self, sentence):\n",
    "        output_label = []\n",
    "        output = []\n",
    "        output_idx =[]\n",
    "\n",
    "\n",
    "        for i, token in enumerate(sentence):\n",
    "            prob = random.random()\n",
    "\n",
    "            if prob < self.mask_ratio:\n",
    "                prob /= self.mask_ratio\n",
    "    \n",
    "                if prob < 0.8:\n",
    "                    output.append(self.special_token_dict['MASK'])\n",
    "                elif prob < 0.9:\n",
    "                    output.append(self.random_selection(self.sessions))\n",
    "                else:\n",
    "                    output.append(token)\n",
    "    \n",
    "                output_label.append(token)\n",
    "                output_idx.append(1)\n",
    "    \n",
    "            else:\n",
    "                output.append(token)\n",
    "                output_label.append(0)\n",
    "                output_idx.append(0)\n",
    "                \n",
    "\n",
    "        assert len(output) == len(output_label)\n",
    "        return output, output_label, output_idx\n",
    "        \n",
    "\n",
    "    def random_selection(self, input_sequences):\n",
    "        rand_session = random.randrange(len(input_sequences))\n",
    "        rand_flow = random.randrange(len(input_sequences[rand_session]))\n",
    "        return input_sequences[rand_session][rand_flow]\n",
    "        \n",
    "\n",
    "    def get_session_flow(self, item):\n",
    "        '''Return session data and segments'''\n",
    "        return self.sessions[item], self.segments[item],self.labels[item]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c5f31cd5-c168-424c-b84e-558ae0d7a34e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_the_final_input_data(input_sequences,input_segments, input_labels):\n",
    "    # Desired length for each piece\n",
    "    MAX_LEN = 2000\n",
    "    # Padding value (you can use any value you prefer)\n",
    "    padding_value = 0\n",
    "\n",
    "    special_tokens = ['PAD', 'MASK']\n",
    "    special_token_dict = {}\n",
    "    for i in range(len(special_tokens)):\n",
    "        special_token_dict[special_tokens[i]] = 1026+i+1\n",
    "    \n",
    "    # print(special_token_dict)\n",
    "\n",
    "    train_data = NetformerDatasetDownstream(input_sequences,input_segments, input_labels,seq_len=MAX_LEN)\n",
    "    return train_data \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7b466479-25b7-4589-a035-154ddbc34b33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('/home/binghui/NDSS2025/Intrution-detection/CICdataset2018/Botnet/Botnet-attack.csv', 1), ('/home/binghui/NDSS2025/Intrution-detection/CICdataset2018/Botnet/Botnet-benign.csv', 0), ('/home/binghui/NDSS2025/Intrution-detection/CICdataset2018/BruteForce/BruteForce-attack.csv', 2), ('/home/binghui/NDSS2025/Intrution-detection/CICdataset2018/BruteForce/BruteForce-benign.csv', 0), ('/home/binghui/NDSS2025/Intrution-detection/CICdataset2018/DDoS/DDoS-attack.csv', 3), ('/home/binghui/NDSS2025/Intrution-detection/CICdataset2018/DDoS/DDoS-benign.csv', 0), ('/home/binghui/NDSS2025/Intrution-detection/CICdataset2018/DoS/DoS-attack.csv', 4), ('/home/binghui/NDSS2025/Intrution-detection/CICdataset2018/DoS/DoS-benign.csv', 0), ('/home/binghui/NDSS2025/Intrution-detection/CICdataset2018/Infiltration/Infiltration-attack.csv', 5), ('/home/binghui/NDSS2025/Intrution-detection/CICdataset2018/Infiltration/Infiltration-benign.csv', 0)]\n"
     ]
    }
   ],
   "source": [
    "# Labeling function for the given dataset\n",
    "def label_data(base_path, attack_labels):\n",
    "    file_list = []\n",
    "    for folder in attack_labels:\n",
    "        attack_file = f\"{base_path}/{folder}/{folder}-attack.csv\"\n",
    "        benign_file = f\"{base_path}/{folder}/{folder}-benign.csv\"\n",
    "        \n",
    "        # Read attack files and label them\n",
    "        df_attack = pd.read_csv(attack_file)\n",
    "        df_attack['label'] = attack_labels[folder]\n",
    "        file_list.append((attack_file, attack_labels[folder]))\n",
    "        \n",
    "        # Read benign files and label them\n",
    "        df_benign = pd.read_csv(benign_file)\n",
    "        df_benign['label'] = 0\n",
    "        file_list.append((benign_file, 0))\n",
    "        \n",
    "    return file_list\n",
    "\n",
    "# Define attack labels\n",
    "attack_labels = {\n",
    "    'Botnet': 1,\n",
    "    'BruteForce': 2,\n",
    "    'DDoS': 3,\n",
    "    'DoS': 4,\n",
    "    'Infiltration': 5\n",
    "}\n",
    "\n",
    "# Base path of your dataset\n",
    "base_path = '/home/binghui/NDSS2025/Intrution-detection/CICdataset2018'\n",
    "\n",
    "# Get the labeled file list\n",
    "file_name_list = label_data(base_path, attack_labels)\n",
    "print(file_name_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0fc8bb0e-0c20-4c3e-b36a-bd95222332e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name_list = [('/home/binghui/NDSS2025/Intrution-detection/CICdataset2018/Botnet/Botnet-attack.csv', 1), ('/home/binghui/NDSS2025/Intrution-detection/CICdataset2018/Botnet/Botnet-benign.csv', 0), ('/home/binghui/NDSS2025/Intrution-detection/CICdataset2018/BruteForce/BruteForce-attack.csv', 2), ('/home/binghui/NDSS2025/Intrution-detection/CICdataset2018/BruteForce/BruteForce-benign.csv', 0), ('/home/binghui/NDSS2025/Intrution-detection/CICdataset2018/DDoS/DDoS-attack.csv', 3), ('/home/binghui/NDSS2025/Intrution-detection/CICdataset2018/DDoS/DDoS-benign.csv', 0), ('/home/binghui/NDSS2025/Intrution-detection/CICdataset2018/DoS/DoS-attack.csv', 4), ('/home/binghui/NDSS2025/Intrution-detection/CICdataset2018/DoS/DoS-benign.csv', 0), ('/home/binghui/NDSS2025/Intrution-detection/CICdataset2018/Infiltration/Infiltration-attack.csv', 5), ('/home/binghui/NDSS2025/Intrution-detection/CICdataset2018/Infiltration/Infiltration-benign.csv', 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9b620494-8aa9-4a43-aee8-506601686113",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processing function, refined and completed\n",
    "def process_and_save(file_name, label, output_path):\n",
    "    print(f\"Processing file: {file_name} with label: {label}\")\n",
    "    df_packet_token, df_flow_token = processing_file(file_name)\n",
    "    input_sequences, input_segments, input_labels = input_generation(df_flow_token, df_packet_token, label)\n",
    "\n",
    "    # Save the processed data to individual files\n",
    "    with open(output_path, 'wb') as f:\n",
    "        pickle.dump((input_sequences, input_segments, input_labels), f)\n",
    "    return output_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8ae61fee-c7b1-4e8d-a225-d1d0cc672a88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: /home/binghui/NDSS2025/Intrution-detection/CICdataset2018/Botnet/Botnet-attack.csv with label: 1\n",
      "Packet Processing Done\n",
      "(138713, 13)\n",
      "Tokenization: Binning\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing sessions: 100%|██████████████████████| 10/10 [00:00<00:00, 20.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing sessions: 100%|██████████████████████| 10/10 [00:00<00:00, 19.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Base path of your dataset\n",
    "base_path = '/home/binghui/NDSS2025/Intrution-detection/CICdataset2018'\n",
    "output_path = '/home/binghui/NDSS2025/Intrution-detection/CICdataset2018/output'\n",
    "final_output_file = '/home/binghui/NDSS2025/Intrution-detection/CICdataset2018/CIC2018-dataset-all.pkl'\n",
    "\n",
    "# # Ensure output directory exists\n",
    "# if not os.path.exists(output_path):\n",
    "#     os.makedirs(output_path)\n",
    "\n",
    "# Process and save each file individually\n",
    "processed_files = []\n",
    "for file_name, label in file_name_list:\n",
    "    output_file = os.path.join(output_path, os.path.basename(file_name).replace('.csv', '.pkl'))\n",
    "    processed_files.append(process_and_save(file_name, label, output_file))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "de8def17-32c3-4b1f-84d1-47d9471036dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/home/binghui/NDSS2025/Intrution-detection/CICdataset2018/output/Botnet-attack.pkl', '/home/binghui/NDSS2025/Intrution-detection/CICdataset2018/output/Botnet-benign.pkl', '/home/binghui/NDSS2025/Intrution-detection/CICdataset2018/output/BruteForce-attack.pkl', '/home/binghui/NDSS2025/Intrution-detection/CICdataset2018/output/BruteForce-benign.pkl', '/home/binghui/NDSS2025/Intrution-detection/CICdataset2018/output/DDoS-attack.pkl', '/home/binghui/NDSS2025/Intrution-detection/CICdataset2018/output/DDoS-benign.pkl', '/home/binghui/NDSS2025/Intrution-detection/CICdataset2018/output/DoS-attack.pkl', '/home/binghui/NDSS2025/Intrution-detection/CICdataset2018/output/DoS-benign.pkl', '/home/binghui/NDSS2025/Intrution-detection/CICdataset2018/output/Infiltration-attack.pkl', '/home/binghui/NDSS2025/Intrution-detection/CICdataset2018/output/Infiltration-benign.pkl']\n"
     ]
    }
   ],
   "source": [
    "final_output_file = '/home/binghui/NDSS2025/Intrution-detection/CICdataset2018/CIC2018-dataset-2-classes.pkl'\n",
    "output_path = '/home/binghui/NDSS2025/Intrution-detection/CICdataset2018/output'\n",
    "processed_files = []\n",
    "for file_name, label in file_name_list:\n",
    "    output_file = os.path.join(output_path, os.path.basename(file_name).replace('.csv', '.pkl'))\n",
    "    processed_files.append(output_file)\n",
    "print(processed_files)\n",
    "# Combining function to concatenate saved data\n",
    "def combine_saved_data(file_paths, output_file):\n",
    "    all_input_sequences, all_input_segments, all_input_labels = [], [], []\n",
    "\n",
    "    for file_path in file_paths:\n",
    "        with open(file_path, 'rb') as f:\n",
    "            input_sequences, input_segments, input_labels = pickle.load(f)\n",
    "            if len(all_input_sequences) == 0:\n",
    "                all_input_sequences = input_sequences\n",
    "                all_input_segments = input_segments\n",
    "                all_input_labels = input_labels\n",
    "            else:\n",
    "                all_input_sequences = np.concatenate((all_input_sequences, input_sequences), axis=0)\n",
    "                all_input_segments = np.concatenate((all_input_segments, input_segments), axis=0)\n",
    "                all_input_labels = np.concatenate((all_input_labels, input_labels), axis=0)\n",
    "    print(np.unique(all_input_labels))\n",
    "    all_input_labels = np.where(all_input_labels == 0, 0, 1)\n",
    "    print(np.unique(all_input_labels))\n",
    "    train_data = NetformerDatasetDownstream(all_input_sequences, all_input_segments, all_input_labels, seq_len=2000)\n",
    "    with open(output_file, 'wb') as f:\n",
    "        pickle.dump(train_data, f)\n",
    "\n",
    "    return output_file\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7d186e92-052b-4b85-b1f2-257daeefcae2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 1. 2. 3. 4. 5.]\n",
      "[0 1]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/home/binghui/NDSS2025/Intrution-detection/CICdataset2018/CIC2018-dataset-2-classes.pkl'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Combine all processed files into one final dataset\n",
    "combine_saved_data(processed_files, final_output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff3fc148-a55e-4a9d-b3f9-2e70ab6f6669",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c46a5ef-5a12-42c2-be7f-a7f87c8546f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "768027d2-fe4f-4423-a26c-9be91d608378",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c860bbd7-43b9-482b-93c9-9c9a7c2317cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87abc7ab-d8a4-4ed4-82cc-e2c63762875a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "86d29c52-83e5-4a62-9d73-cad075b441c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Model_input(file_name_list):\n",
    "    MAX_LEN = 2000\n",
    "    # Padding value (you can use any value you prefer)\n",
    "    padding_value = 0\n",
    "\n",
    "    special_tokens = ['PAD', 'MASK']\n",
    "    special_token_dict = {}\n",
    "    for i in range(len(special_tokens)):\n",
    "        special_token_dict[special_tokens[i]] = 1026+i+1\n",
    "    \n",
    "    all_input_sequences, all_input_segments,all_input_labels = [],[],[]\n",
    "    for file_name, label in file_name_list:\n",
    "        print(f\"Processing file: {file_name} with label: {label}\")\n",
    "        df_packet_token, df_flow_token = processing_file(file_name)\n",
    "        input_sequences, input_segments, input_labels = input_generation(df_flow_token, df_packet_token, label)\n",
    "        if len(all_input_sequences) == 0:\n",
    "            all_input_sequences = input_sequences\n",
    "            all_input_segments = input_segments\n",
    "            all_input_labels = input_labels\n",
    "        else:\n",
    "            all_input_sequences = np.concatenate((all_input_sequences, input_sequences), axis=0)\n",
    "            all_input_segments = np.concatenate((all_input_segments, input_segments), axis=0)\n",
    "            all_input_labels = np.concatenate((all_input_labels, input_labels), axis=0)\n",
    "    \n",
    "    train_data = NetformerDatasetDownstream(all_input_sequences, all_input_segments, all_input_labels, seq_len=MAX_LEN)\n",
    "    with open('CIC2018-dataset-all.pkl', 'wb') as f:\n",
    "        pickle.dump(train_data, f)\n",
    "    return 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e314e59-0cda-4a61-953f-8118e8eb4884",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: /home/binghui/NDSS2025/Intrution-detection/CICdataset2018/Botnet/Botnet-attack.csv with label: 1\n",
      "Packet Processing Done\n",
      "(138713, 13)\n",
      "Tokenization: Binning\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing sessions: 100%|██████████████████████| 10/10 [00:00<00:00, 20.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing sessions: 100%|██████████████████████| 10/10 [00:00<00:00, 22.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing complete.\n",
      "Processing file: /home/binghui/NDSS2025/Intrution-detection/CICdataset2018/Botnet/Botnet-benign.csv with label: 0\n",
      "Packet Processing Done\n",
      "(56507, 13)\n",
      "Tokenization: Binning\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing sessions: 100%|████████████████| 1216/1216 [00:00<00:00, 7401.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing sessions: 100%|████████████████| 1216/1216 [00:00<00:00, 7687.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing complete.\n",
      "Processing file: /home/binghui/NDSS2025/Intrution-detection/CICdataset2018/BruteForce/BruteForce-attack.csv with label: 2\n",
      "Packet Processing Done\n",
      "(28262, 13)\n",
      "Tokenization: Binning\n"
     ]
    }
   ],
   "source": [
    "Model_input(file_name_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f14ab9c3-7d18-42b6-b502-62cbb85d088e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c11dc1cf-0e04-4b91-b2bf-b8a3644d5eb7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aacaeb52-63c7-4d31-87ad-9362864b89ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63977444-495a-4201-91b4-2f6a50a333b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
