{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6fd22319-e8e8-4ae8-882e-0fbfbd5a43ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import re\n",
    "import random\n",
    "import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import itertools\n",
    "import math\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torch.optim import Adam\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import pickle\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "## for cut length\n",
    "from itertools import zip_longest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a0a07339-88e2-41ab-8d40-4eedb99b6fcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Jul  9 12:31:49 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.183.01             Driver Version: 535.183.01   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA GeForce RTX 4080        Off | 00000000:01:00.0  On |                  N/A |\n",
      "|  0%   42C    P8              13W / 320W |    963MiB / 16376MiB |     16%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A      2200      G   /usr/lib/xorg/Xorg                          210MiB |\n",
      "|    0   N/A  N/A      2416      G   /usr/bin/gnome-shell                         37MiB |\n",
      "|    0   N/A  N/A      4105      G   ...seed-version=20240707-180341.351000      221MiB |\n",
      "|    0   N/A  N/A     19799      C   ...da3/envs/backdoor-attack/bin/python      238MiB |\n",
      "|    0   N/A  N/A     25248      C   ...da3/envs/backdoor-attack/bin/python      238MiB |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# for Google colab: check for gpu\n",
    "gpu_info = !nvidia-smi\n",
    "gpu_info = '\\n'.join(gpu_info)\n",
    "if gpu_info.find('failed') >= 0:\n",
    "  print('Not connected to a GPU')\n",
    "else:\n",
    "  print(gpu_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ac2268b-4b07-48d5-85d3-6ac310680ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "### absolute positional encoding\n",
    "class PositionalEmbedding(torch.nn.Module):\n",
    "    # d_model is the dimension of output : 300\n",
    "    def __init__(self, d_model, max_len=2000):\n",
    "        super().__init__()\n",
    "\n",
    "        # Compute the positional encodings once in log space.\n",
    "        pe = torch.zeros(max_len, d_model).float() # 2000 * 1030\n",
    "        pe.require_grad = False\n",
    "\n",
    "        for pos in range(max_len):\n",
    "            # for each dimension of the each position\n",
    "            for i in range(0, d_model, 2):\n",
    "                pe[pos, i] = math.sin(pos / (10000 ** ((2 * i)/d_model)))\n",
    "                pe[pos, i + 1] = math.cos(pos / (10000 ** ((2 * (i + 1))/d_model)))\n",
    "        # include the batch size\n",
    "        self.register_buffer('pe', pe.unsqueeze(0))\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.pe\n",
    "\n",
    "\n",
    "\n",
    "## embeddings : positional + segmentation + packet embedding\n",
    "class NetformerEmbedding(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Netformer Embedding which is consisted with under features\n",
    "        1. PacketEmbedding : normal embedding matrix\n",
    "        2. PositionalEmbedding : adding positional information using sin, cos\n",
    "        2. SegmentEmbedding : adding [0 : flow, 1 : packet, 2: '[SEP]']\n",
    "        sum of all these features are output of NetformerEmbedding\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, feature_size, embed_size, seq_len=2000, dropout=0.1):\n",
    "        \"\"\"\n",
    "        :param feature_size: total bin size 1030\n",
    "        :param embed_ size: embedding size of token embedding\n",
    "        :param dropout: dropout rate\n",
    "        :seq_len : length of the input features\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.embed_size = embed_size\n",
    "        # (m, seq_len) --> (m, seq_len, embed_size)\n",
    "        # padding_idx is not updated during training, remains as fixed pad (0)\n",
    "        self.token = torch.nn.Embedding(feature_size, embed_size)\n",
    "        self.segment = torch.nn.Embedding(2, embed_size)\n",
    "        self.position = PositionalEmbedding(d_model=embed_size, max_len=seq_len)\n",
    "        self.dropout = torch.nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, sequence, segment_label):\n",
    "\n",
    "        sequence = sequence.to(torch.long)\n",
    "        segment_label= segment_label.to(torch.long)\n",
    "        # print(segment_label.shape)\n",
    "        # print(sequence.shape)\n",
    "        # print(self.token(sequence).shape)\n",
    "        # print(self.position(sequence).shape)\n",
    "        # print(self.segment(segment_label).shape)\n",
    "        # print(\"Max index in sequence:\", sequence.max())\n",
    "        # print(\"Min index in sequence:\", sequence.min())\n",
    "        # print(\"Max index in segment_label:\", segment_label.max())\n",
    "        # print(\"Min index in segment_label:\", segment_label.min())\n",
    "        x = self.token(sequence) + self.position(sequence) + self.segment(segment_label)\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf58888-c9e7-499d-94b8-f6a781d3b4aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8071a8bb-7f36-4877-a294-e71ee81f19f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "### multi-head attention\n",
    "class MultiHeadedAttention(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, heads, d_model, dropout=0.1):\n",
    "        super(MultiHeadedAttention, self).__init__()\n",
    "\n",
    "        assert d_model % heads == 0\n",
    "        self.d_k = d_model // heads\n",
    "        self.heads = heads\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "\n",
    "        self.query = torch.nn.Linear(d_model, d_model)\n",
    "        self.key = torch.nn.Linear(d_model, d_model)\n",
    "        self.value = torch.nn.Linear(d_model, d_model)\n",
    "        self.output_linear = torch.nn.Linear(d_model, d_model)\n",
    "\n",
    "    def forward(self, query, key, value, mask):\n",
    "        \"\"\"\n",
    "        query, key, value of shape: (batch_size, max_len, d_model)\n",
    "        mask of shape: (batch_size, 1, feature_size, max_words)\n",
    "        \"\"\"\n",
    "        # (batch_size, max_len, d_model)\n",
    "        query = self.query(query)\n",
    "        key = self.key(key)\n",
    "        value = self.value(value)\n",
    "\n",
    "        # (batch_size, max_len, d_model) --> (batch_size, max_len, h, d_k) --> (batch_size, h, max_len, d_k)\n",
    "        query = query.view(query.shape[0], -1, self.heads, self.d_k).permute(0, 2, 1, 3)\n",
    "        key = key.view(key.shape[0], -1, self.heads, self.d_k).permute(0, 2, 1, 3)\n",
    "        value = value.view(value.shape[0], -1, self.heads, self.d_k).permute(0, 2, 1, 3)\n",
    "\n",
    "        # (batch_size, h, max_len, d_k) matmul (batch_size, h, d_k, max_len) --> (batch_size, h, max_len, max_len)\n",
    "        scores = torch.matmul(query, key.permute(0, 1, 3, 2)) / math.sqrt(query.size(-1))\n",
    "\n",
    "        # fill 0 mask with super small number so it wont affect the softmax weight\n",
    "        # (batch_size, h, max_len, max_len)\n",
    "        scores = scores.masked_fill(mask == 0, -1e9)\n",
    "\n",
    "        # (batch_size, h, max_len, max_len)\n",
    "        # softmax to put attention weight for all non-pad tokens\n",
    "        # max_len X max_len matrix of attention\n",
    "        weights = F.softmax(scores, dim=-1)\n",
    "        weights = self.dropout(weights)\n",
    "\n",
    "        # (batch_size, h, max_len, max_len) matmul (batch_size, h, max_len, d_k) --> (batch_size, h, max_len, d_k)\n",
    "        context = torch.matmul(weights, value)\n",
    "\n",
    "        # (batch_size, h, max_len, d_k) --> (batch_size, max_len, h, d_k) --> (batch_size, max_len, d_model)\n",
    "        context = context.permute(0, 2, 1, 3).contiguous().view(context.shape[0], -1, self.heads * self.d_k)\n",
    "\n",
    "        # (batch_size, max_len, d_model)\n",
    "        return self.output_linear(context)\n",
    "\n",
    "class FeedForward(torch.nn.Module):\n",
    "    \"Implements FFN equation.\"\n",
    "\n",
    "    def __init__(self, d_model, middle_dim=500, dropout=0.1):\n",
    "        super(FeedForward, self).__init__()\n",
    "\n",
    "        self.fc1 = torch.nn.Linear(d_model, middle_dim)\n",
    "        self.fc2 = torch.nn.Linear(middle_dim, d_model)\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "        self.activation = torch.nn.GELU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.activation(self.fc1(x))\n",
    "        out = self.fc2(self.dropout(out))\n",
    "        return out\n",
    "\n",
    "class EncoderLayer(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model=10,\n",
    "        heads=10,\n",
    "        feed_forward_hidden=10 * 4,\n",
    "        dropout=0.1\n",
    "        ):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.layernorm = torch.nn.LayerNorm(d_model)\n",
    "        self.self_multihead = MultiHeadedAttention(heads, d_model)\n",
    "        self.feed_forward = FeedForward(d_model, middle_dim=feed_forward_hidden)\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, embeddings, mask):\n",
    "        # embeddings: (batch_size, max_len, d_model)\n",
    "        # encoder mask: (batch_size, 1, 1, max_len)\n",
    "        # result: (batch_size, max_len, d_model)\n",
    "        interacted = self.dropout(self.self_multihead(embeddings, embeddings, embeddings, mask))\n",
    "        # residual layer\n",
    "        interacted = self.layernorm(interacted + embeddings)\n",
    "        # bottleneck\n",
    "        feed_forward_out = self.dropout(self.feed_forward(interacted))\n",
    "        encoded = self.layernorm(feed_forward_out + interacted)\n",
    "        return encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b37b01c-04d9-43a4-a2c4-9810c753b0aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetFormer(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Netformer model : Native transformer for network traffic\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, feature_size = 1030, d_model=10, n_layers=2, heads=2, dropout=0.1, number_of_class=4):\n",
    "        \"\"\"\n",
    "        :param feature_size: feature_size of total bins\n",
    "        :param hidden : hidden layers of Netformer\n",
    "        :param n_layers: numbers of Transformer blocks(layers)\n",
    "        :param attn_heads: number of attention heads\n",
    "        :param dropout: dropout rate\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.n_layers = n_layers\n",
    "        self.heads = heads\n",
    "        self.number_of_class = number_of_class\n",
    "        # paper noted they used 4 * hidden_size for ff_network_hidden_size\n",
    "        self.feed_forward_hidden = d_model * 4\n",
    "        # self.classhead = Classification_head(d_model, number_of_class)\n",
    "        # embedding for Netformer, sum of positional, segment, token embeddings\n",
    "        self.embedding = NetformerEmbedding(feature_size = feature_size, embed_size=d_model)\n",
    "        \n",
    "        # multi-layers transformer blocks, deep network\n",
    "        self.encoder_blocks = torch.nn.ModuleList(\n",
    "            [EncoderLayer(d_model, heads, d_model * 4, dropout) for _ in range(n_layers)])\n",
    "\n",
    "    def forward(self, x, segment_info):\n",
    "        # attention masking for padded token\n",
    "        # print(segment_info.dim())\n",
    "        assert segment_info.dim() == 2, \"segment_info should have 2 dimensions: [batch_size, seq_len]\"\n",
    "        \n",
    "        # (batch_size, 1, seq_len, seq_len)\n",
    "        # mask = (segment_info > 0).unsqueeze(1).repeat(1, segment_info.size(1), 1).unsqueeze(1)\n",
    "        mask = (segment_info > 0).unsqueeze(1).repeat(1, segment_info.size(1), 1).unsqueeze(1)\n",
    "\n",
    "        # embedding the indexed sequence to sequence of vectors\n",
    "        x = self.embedding(x, segment_info)\n",
    "        # print(\"embedding works\")\n",
    "        # running over multiple transformer blocks\n",
    "        for encoder in self.encoder_blocks:\n",
    "            x = encoder.forward(x, mask)\n",
    "        # print(\"encoder works\")\n",
    "        # x = self.self.classhead(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "93551660-dd37-4791-b940-fd4a6d0bab1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classification_head(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    for classification head define\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, hidden, num_of_classes,drop=0.):\n",
    "        super().__init__()\n",
    "        self.layer1 = torch.nn.Linear(hidden, 1)\n",
    "        self.mlp_head = torch.nn.Linear(2000, num_of_classes)\n",
    "        self.softmax = torch.nn.LogSoftmax(dim=-1)\n",
    "\n",
    "        # self.act1 = torch.nn.GELU()\n",
    "        # self.drop =torch.nn.Dropout(drop)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.layer1(x)\n",
    "        # print(x.shape)\n",
    "        x = x.squeeze()\n",
    "        # print(x.shape)\n",
    "        x = self.mlp_head(x)\n",
    "        # print(x.shape)\n",
    "        x = self.softmax(x)\n",
    "        # print(x.shape)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2b99f3f6-db5c-4b89-9889-d4621a348640",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetFormerLM(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    BERT Language Model\n",
    "    Next Sentence Prediction Model + Masked Language Model\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, netformer: NetFormer, number_of_class):\n",
    "        \"\"\"\n",
    "        :param netformer: netformer model which should be trained\n",
    "        :param vocab_size: total vocab size for masked_lm\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "        self.netformer = netformer\n",
    "        self.chead = Classification_head(self.netformer.d_model, number_of_class)\n",
    "\n",
    "    def forward(self, x, segment_label):\n",
    "        x = self.netformer(x, segment_label)\n",
    "        return self.chead(x)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## schedule learning rate\n",
    "\n",
    "class ScheduledOptim():\n",
    "    '''A simple wrapper class for learning rate scheduling'''\n",
    "\n",
    "    def __init__(self, optimizer, d_model, n_warmup_steps):\n",
    "        self._optimizer = optimizer\n",
    "        self.n_warmup_steps = n_warmup_steps\n",
    "        self.n_current_steps = 0\n",
    "        self.init_lr = np.power(d_model, -0.5)\n",
    "\n",
    "    def step_and_update_lr(self):\n",
    "        \"Step with the inner optimizer\"\n",
    "        self._update_learning_rate()\n",
    "        self._optimizer.step()\n",
    "\n",
    "    def zero_grad(self):\n",
    "        \"Zero out the gradients by the inner optimizer\"\n",
    "        self._optimizer.zero_grad()\n",
    "\n",
    "    def _get_lr_scale(self):\n",
    "        return np.min([\n",
    "            np.power(self.n_current_steps, -0.5),\n",
    "            np.power(self.n_warmup_steps, -1.5) * self.n_current_steps])\n",
    "\n",
    "    def _update_learning_rate(self):\n",
    "        ''' Learning rate scheduling per step '''\n",
    "\n",
    "        self.n_current_steps += 1\n",
    "        lr = self.init_lr * self._get_lr_scale()\n",
    "\n",
    "        for param_group in self._optimizer.param_groups:\n",
    "            param_group['lr'] = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5f90a367-e8b5-4b87-9384-750e88ab427f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## trainner\n",
    "# class NetformerTrainer:\n",
    "#     def __init__(\n",
    "#         self,\n",
    "#         model,\n",
    "#         train_dataloader,\n",
    "#         test_dataloader=None,\n",
    "#         lr= 1e-4,\n",
    "#         weight_decay=0.01,\n",
    "#         betas=(0.9, 0.999),\n",
    "#         warmup_steps=1000,\n",
    "#         log_freq=200,\n",
    "#         num_of_class = 6,\n",
    "#         device='cuda'\n",
    "#         ):\n",
    "\n",
    "#         self.device = device\n",
    "#         self.model = model\n",
    "#         self.train_data = train_dataloader\n",
    "#         self.test_data = test_dataloader\n",
    "#         self.num_of_class = num_of_class\n",
    "\n",
    "#         # Setting the Adam optimizer with hyper-param\n",
    "#         self.optim = Adam(self.model.parameters(), lr=lr, betas=betas, weight_decay=weight_decay)\n",
    "#         self.optim_schedule = ScheduledOptim(\n",
    "#             self.optim, self.model.netformer.d_model, n_warmup_steps=warmup_steps\n",
    "#             )\n",
    "\n",
    "#         # Using Negative Log Likelihood Loss function for predicting the masked_token\n",
    "#         self.criterion = torch.nn.CrossEntropyLoss()\n",
    "#         # self.criterion_2 = torch.nn.MSELoss()\n",
    "#         self.log_freq = log_freq\n",
    "#         print(\"Total Parameters:\", sum([p.nelement() for p in self.model.parameters()]))\n",
    "\n",
    "#     def train(self, epoch):\n",
    "#         self.iteration(epoch, self.train_data)\n",
    "\n",
    "#     def test(self, epoch):\n",
    "#         self.iteration(epoch, self.test_data, train=False)\n",
    "\n",
    "#     def iteration(self, epoch, data_loader, train=True):\n",
    "\n",
    "#         avg_loss = 0.0\n",
    "#         total_correct = 0\n",
    "#         total_element = 0\n",
    "#         all_labels = []\n",
    "#         all_preds = []\n",
    "#         mode = \"train\" if train else \"test\"\n",
    "\n",
    "#         # progress bar\n",
    "#         data_iter = tqdm.tqdm(\n",
    "#             enumerate(data_loader),\n",
    "#             desc=\"EP_%s:%d\" % (mode, epoch),\n",
    "#             total=len(data_loader),\n",
    "#             bar_format=\"{l_bar}{r_bar}\"\n",
    "#         )\n",
    "\n",
    "#         for i, data in data_iter:\n",
    "\n",
    "#             # 0. batch_data will be sent into the device(GPU or cpu)\n",
    "#             data = {key: value.to(self.device) for key, value in data.items()}\n",
    "\n",
    "#             # 1. forward the next_sentence_prediction and masked_lm model\n",
    "#             classification_output = self.model.forward(data[\"netformer_input\"], data[\"segment_label\"])\n",
    "#             # class_output = mask_lm_output.transpose(1, 2)\n",
    "#             # print(mask_lm_output.shape)\n",
    "#             # print(type(mask_lm_output))\n",
    "#             # print(\"classification_output shape:\", classification_output.shape)\n",
    "#             # print(\"sequence_label shape:\", data[\"sequence_label\"].shape)\n",
    "\n",
    "#             classification_loss = self.criterion(classification_output, data[\"sequence_label\"].to(torch.long))\n",
    "\n",
    "#             # 2-3. Adding next_loss and mask_loss : 3.4 Pre-training Procedure\n",
    "#             loss = classification_loss \n",
    "\n",
    "#             # 3. backward and optimization only in train\n",
    "#             if train:\n",
    "#                 self.optim_schedule.zero_grad()\n",
    "#                 loss.backward()\n",
    "#                 self.optim_schedule.step_and_update_lr()\n",
    "\n",
    "#             correct = classification_output.argmax(dim=-1).eq(data[\"sequence_label\"]).sum().item()\n",
    "#             total_correct += correct\n",
    "#             total_element += data[\"sequence_label\"].nelement()\n",
    "\n",
    "#             all_labels.extend(data[\"sequence_label\"].cpu().numpy())\n",
    "#             all_preds.extend(classification_output.argmax(dim=-1).cpu().numpy())\n",
    "            \n",
    "#             post_fix = {\n",
    "#                 \"epoch\": epoch,\n",
    "#                 \"iter\": i,\n",
    "#                 # \"avg_loss\": avg_loss / (i + 1),\n",
    "#                 # \"avg_acc\": total_correct / total_element * 100,\n",
    "#                 \"loss\": loss.item()\n",
    "#             }\n",
    "\n",
    "#             if i % self.log_freq == 0:\n",
    "#                 data_iter.write(str(post_fix))\n",
    "#         print( \"total_acc=\",\n",
    "#               total_correct * 100.0 / total_element)\n",
    "\n",
    "#         # Calculate metrics\n",
    "#         all_labels = np.array(all_labels)\n",
    "#         all_preds = np.array(all_preds)\n",
    "        \n",
    "#         accuracy = accuracy_score(all_labels, all_preds)\n",
    "#         precision = precision_score(all_labels, all_preds, average=None, zero_division=0)\n",
    "#         recall = recall_score(all_labels, all_preds, average=None, zero_division=0)\n",
    "#         f1 = f1_score(all_labels, all_preds, average=None, zero_division=0)\n",
    "#         # roc_auc = roc_auc_score(all_labels, self.one_hot_encode(all_preds, self.num_of_class), average=None, multi_class='ovr')\n",
    "\n",
    "#         print(f\"Accuracy: {accuracy:.4f}\")\n",
    "#         for i in range(self.num_of_class):\n",
    "#             print(f\"Class {i}: Precision: {precision[i]:.4f}, Recall: {recall[i]:.4f}, F1-Score: {f1[i]:.4f}, ROC-AUC: {roc_auc[i]:.4f}\")\n",
    "\n",
    "#     @staticmethod\n",
    "#     def one_hot_encode(labels, num_classes):\n",
    "#         return np.eye(num_classes)[labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "097b1de0-a2a1-4587-aec4-a15bb1ce87da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import confusion_matrix\n",
    "# import numpy as np\n",
    "\n",
    "class NetformerTrainer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model,\n",
    "        train_dataloader,\n",
    "        test_dataloader=None,\n",
    "        lr= 1e-4,\n",
    "        weight_decay=0.01,\n",
    "        betas=(0.9, 0.999),\n",
    "        warmup_steps=1000,\n",
    "        log_freq=200,\n",
    "        num_of_class = 6,\n",
    "        device='cuda'\n",
    "        ):\n",
    "\n",
    "        self.device = device\n",
    "        self.model = model\n",
    "        self.train_data = train_dataloader\n",
    "        self.test_data = test_dataloader\n",
    "        self.num_of_class = num_of_class\n",
    "\n",
    "        # Setting the Adam optimizer with hyper-param\n",
    "        self.optim = Adam(self.model.parameters(), lr=lr, betas=betas, weight_decay=weight_decay)\n",
    "        self.optim_schedule = ScheduledOptim(\n",
    "            self.optim, self.model.netformer.d_model, n_warmup_steps=warmup_steps\n",
    "            )\n",
    "\n",
    "        # Using Negative Log Likelihood Loss function for predicting the masked_token\n",
    "        self.criterion = torch.nn.CrossEntropyLoss()\n",
    "        self.log_freq = log_freq\n",
    "        print(\"Total Parameters:\", sum([p.nelement() for p in self.model.parameters()]))\n",
    "\n",
    "    def train(self, epoch):\n",
    "        self.iteration(epoch, self.train_data)\n",
    "\n",
    "    def test(self, epoch):\n",
    "        self.iteration(epoch, self.test_data, train=False)\n",
    "\n",
    "    def iteration(self, epoch, data_loader, train=True):\n",
    "\n",
    "        avg_loss = 0.0\n",
    "        total_correct = 0\n",
    "        total_element = 0\n",
    "        all_labels = []\n",
    "        all_preds = []\n",
    "        mode = \"train\" if train else \"test\"\n",
    "\n",
    "        # progress bar\n",
    "        data_iter = tqdm.tqdm(\n",
    "            enumerate(data_loader),\n",
    "            desc=\"EP_%s:%d\" % (mode, epoch),\n",
    "            total=len(data_loader),\n",
    "            bar_format=\"{l_bar}{r_bar}\"\n",
    "        )\n",
    "\n",
    "        for i, data in data_iter:\n",
    "\n",
    "            # 0. batch_data will be sent into the device(GPU or cpu)\n",
    "            data = {key: value.to(self.device) for key, value in data.items()}\n",
    "\n",
    "            # 1. forward the next_sentence_prediction and masked_lm model\n",
    "            classification_output = self.model.forward(data[\"netformer_input\"], data[\"segment_label\"])\n",
    "            classification_loss = self.criterion(classification_output, data[\"sequence_label\"].to(torch.long))\n",
    "\n",
    "            # 2-3. Adding next_loss and mask_loss : 3.4 Pre-training Procedure\n",
    "            loss = classification_loss \n",
    "\n",
    "            # 3. backward and optimization only in train\n",
    "            if train:\n",
    "                self.optim_schedule.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optim_schedule.step_and_update_lr()\n",
    "\n",
    "            correct = classification_output.argmax(dim=-1).eq(data[\"sequence_label\"]).sum().item()\n",
    "            total_correct += correct\n",
    "            total_element += data[\"sequence_label\"].nelement()\n",
    "\n",
    "            all_labels.extend(data[\"sequence_label\"].cpu().numpy())\n",
    "            all_preds.extend(classification_output.argmax(dim=-1).cpu().numpy())\n",
    "            \n",
    "            post_fix = {\n",
    "                \"epoch\": epoch,\n",
    "                \"iter\": i,\n",
    "                \"loss\": loss.item()\n",
    "            }\n",
    "\n",
    "            if i % self.log_freq == 0:\n",
    "                data_iter.write(str(post_fix))\n",
    "        print(\"total_acc=\", total_correct * 100.0 / total_element)\n",
    "\n",
    "        # Calculate metrics\n",
    "        all_labels = np.array(all_labels)\n",
    "        all_preds = np.array(all_preds)\n",
    "        \n",
    "        accuracy = accuracy_score(all_labels, all_preds)\n",
    "        precision = precision_score(all_labels, all_preds, average=None, zero_division=0)\n",
    "        recall = recall_score(all_labels, all_preds, average=None, zero_division=0)\n",
    "        f1 = f1_score(all_labels, all_preds, average=None, zero_division=0)\n",
    "\n",
    "        print(f\"Accuracy: {accuracy:.4f}\")\n",
    "        for i in range(self.num_of_class):\n",
    "            print(f\"Class {i}: Precision: {precision[i]:.4f}, Recall: {recall[i]:.4f}, F1-Score: {f1[i]:.4f}\")\n",
    "\n",
    "        # Calculate and print confusion matrix\n",
    "        conf_matrix = confusion_matrix(all_labels, all_preds)\n",
    "        print(\"Confusion Matrix:\")\n",
    "        print(conf_matrix)\n",
    "\n",
    "    @staticmethod\n",
    "    def one_hot_encode(labels, num_classes):\n",
    "        return np.eye(num_classes)[labels]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5e5d96bc-8114-4a42-ba06-5c5001e06146",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetformerDatasetDownstream(Dataset):\n",
    "    def __init__(self, input_sequences, input_labels, input_segments, seq_len = 2000):\n",
    "        self.seq_len = seq_len\n",
    "        self.session_flows = len(input_sequences)\n",
    "        self.sessions = input_sequences\n",
    "        self.segments = input_segments\n",
    "        self.labels = input_labels\n",
    "        self.special_token_dict =  {'PAD': 0, 'MASK': 1028}\n",
    "        self.mask_ratio = 0\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.session_flows\n",
    "\n",
    "    def __getitem__(self,item):\n",
    "\n",
    "        ##step 1 : get random sessions \n",
    "        s1, seg1,seq_label = self.get_session_flow(item)\n",
    "\n",
    "        ## step 2: replace random word in sentence \n",
    "        s1_random, s1_label, s1_idx = self.random_word(s1)\n",
    "        \n",
    "        segment_label = seg1\n",
    "\n",
    "        netformer_input = s1_random\n",
    "        netformer_label = s1_label\n",
    "        netformer_idx = s1_idx\n",
    "\n",
    "        \n",
    "        output = {\"netformer_input\": netformer_input,\n",
    "                  \"netformer_label\": netformer_label,\n",
    "                  \"netformer_idx\":netformer_idx,\n",
    "                  \"segment_label\": segment_label,\n",
    "                \"sequence_label\": seq_label}\n",
    "\n",
    "        return {key: torch.tensor(value,dtype=torch.float32) for key, value in output.items()}\n",
    "\n",
    "\n",
    "    def random_word(self, sentence):\n",
    "        output_label = []\n",
    "        output = []\n",
    "        output_idx =[]\n",
    "\n",
    "\n",
    "        for i, token in enumerate(sentence):\n",
    "            prob = random.random()\n",
    "\n",
    "            if prob < self.mask_ratio:\n",
    "                prob /= self.mask_ratio\n",
    "    \n",
    "                if prob < 0.8:\n",
    "                    output.append(self.special_token_dict['MASK'])\n",
    "                elif prob < 0.9:\n",
    "                    output.append(self.random_selection(self.sessions))\n",
    "                else:\n",
    "                    output.append(token)\n",
    "    \n",
    "                output_label.append(token)\n",
    "                output_idx.append(1)\n",
    "    \n",
    "            else:\n",
    "                output.append(token)\n",
    "                output_label.append(0)\n",
    "                output_idx.append(0)\n",
    "                \n",
    "\n",
    "        assert len(output) == len(output_label)\n",
    "        return output, output_label, output_idx\n",
    "        \n",
    "\n",
    "    def random_selection(self, input_sequences):\n",
    "        rand_session = random.randrange(len(input_sequences))\n",
    "        rand_flow = random.randrange(len(input_sequences[rand_session]))\n",
    "        return input_sequences[rand_session][rand_flow]\n",
    "        \n",
    "\n",
    "    def get_session_flow(self, item):\n",
    "        '''Return session data and segments'''\n",
    "        return self.sessions[item], self.segments[item],self.labels[item]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d9cbd7e-63f0-4332-bda9-896b468b5137",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "989a1bfe-3283-444e-8d04-8a1b8bfa512e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
