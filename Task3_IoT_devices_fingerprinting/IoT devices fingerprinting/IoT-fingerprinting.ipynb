{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a2f00989-ab81-4b38-862c-f78723518241",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Aug 20 19:44:25 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.183.01             Driver Version: 535.183.01   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA GeForce RTX 4080        Off | 00000000:01:00.0  On |                  N/A |\n",
      "|  0%   46C    P2              57W / 320W |   8247MiB / 16376MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A      2235      G   /usr/lib/xorg/Xorg                          200MiB |\n",
      "|    0   N/A  N/A      2341    C+G   ...libexec/gnome-remote-desktop-daemon      239MiB |\n",
      "|    0   N/A  N/A      2395      G   /usr/bin/gnome-shell                         57MiB |\n",
      "|    0   N/A  N/A     80567      G   ...seed-version=20240809-130208.212000      158MiB |\n",
      "|    0   N/A  N/A    242343      C   ...da3/envs/backdoor-attack/bin/python     2328MiB |\n",
      "|    0   N/A  N/A    252044      C   ...da3/envs/backdoor-attack/bin/python     5240MiB |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import Tensor\n",
    "from torchsummary import summary\n",
    "import random\n",
    "from numpy import load\n",
    "from tqdm import tqdm, trange\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "%run './Attention_based_model.ipynb'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac4d6adc-d767-4495-a254-698ddf5334ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bdfbf7a9-cecf-4ad4-8887-db98eb06d85d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device:  cuda (NVIDIA GeForce RTX 4080)\n"
     ]
    }
   ],
   "source": [
    "# check the availability of cuda\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device: \", device, f\"({torch.cuda.get_device_name(device)})\" if torch.cuda.is_available() else \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c3f361fa-49fc-4902-b3f1-3fb881343fe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of train windws: (1071459, 200, 8)\n",
      "shape of train labels: (1071459, 1, 28)\n",
      "shape of test windows: (267865, 200, 8)\n",
      "shape of test labels: (267865, 1, 28)\n"
     ]
    }
   ],
   "source": [
    "def load_data_make_split(npz_file, train_percentage):\n",
    "    \"\"\"\n",
    "    Load training data (windows + one-hot labels) from compressed file. Split data into train and test set\n",
    "    Arguments:\n",
    "        - npz_file: The path to the *.npz file\n",
    "        - train_percentage: the percentage of data used for training (and not testing), e.g. 0.8\n",
    "    Returns:\n",
    "        A 4-tuple of train and test data with labels: (x_train, y_train, x_test, y_test)\n",
    "    \"\"\"\n",
    "    dict_data = load(npz_file)\n",
    "    x = dict_data['x']\n",
    "    y = dict_data['y']\n",
    "    train_length = int(len(x)*train_percentage)\n",
    "    x_train = x[:train_length]\n",
    "    y_train = y[:train_length]\n",
    "    x_test = x[train_length:]\n",
    "    y_test = y[train_length:]\n",
    "    return (x_train, y_train, x_test, y_test)\n",
    "\n",
    "# test load_data_make_split()\n",
    "x_train, y_train, x_test, y_test = load_data_make_split(\"update_new_feature_all_days_all_devices.npz\", 0.8)\n",
    "print(\"shape of train windws: {}\".format(x_train.shape))\n",
    "print(\"shape of train labels: {}\".format(y_train.shape))\n",
    "print(\"shape of test windows: {}\".format(x_test.shape))\n",
    "print(\"shape of test labels: {}\".format(y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d93b28e2-d19f-41a5-a34b-fcb97464ef01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(24, 12041), (15, 12270), (6, 14034), (14, 15738), (2, 34720), (0, 36454), (27, 55804), (7, 61729), (1, 67218), (4, 68597), (10, 102906), (19, 111726), (22, 197876), (23, 228604), (5, 275855)]\n"
     ]
    }
   ],
   "source": [
    "## check number of devices in a dataset\n",
    "all_labels = []\n",
    "for i in range(len(y_train)):\n",
    "  index = np.where(y_train[i][0] == True)\n",
    "  k = index[0][0]\n",
    "  all_labels.append(k)\n",
    "\n",
    "for i in range(len(y_test)):\n",
    "  index = np.where(y_test[i][0] == True)\n",
    "  k = index[0][0]\n",
    "  all_labels.append(k)\n",
    "\n",
    "unique, count = np.unique(all_labels, return_counts = True)\n",
    "useable_data = []\n",
    "data_pair =dict()\n",
    "\n",
    "# select the devices data points > 10000 as the supervised pretraining data\n",
    "for i in range(len(unique)):\n",
    "  if count[i] > 10000:\n",
    "    useable_data.append(unique[i])\n",
    "    # print(unique[i], count[i])\n",
    "    data_pair[unique[i]] = count[i]\n",
    "sorted_data_by_counts = sorted(data_pair.items(), key=lambda x:x[1])\n",
    "print(sorted_data_by_counts)\n",
    "\n",
    "\n",
    "all_device_names_dict = {0:'Smart Things', 1: 'Amazon Echo', 2:'Netatmo Welcome',3:'TP-Link Day Night Cloud camera', 4:'Samsung SmartCam', 5: 'Dropcam', 6: 'Withings Smart Baby Monitor', 7:'Belkin Wemo switch', 8:'TP-Link Smart plug',\n",
    "                         9: 'iHome', 10:'Belkin wemo motion sensor', 11:'NEST Protect smoke alarm', 12:'Netatmo weather station',13:'Withings Smart scale',14:'Withings Aura smart sleep sensor',15:'Light Bulbs LiFX Smart Bulb',\n",
    "                         16: 'Triby Speaker', 17:'PIX-STAR Photo-frame', 18 : 'HP Printer', 19: 'Samsung Galaxy Tab', 20: 'Nest Dropcam', 21:'Android Phone', 22:'Laptop', 23:'MacBook', 24:'Android Phone',\n",
    "                         25: 'IPhone', 26:'MacBook/Iphone', 27:'Insteon Camera'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5436a780-2c54-441d-b67f-5353bbef7b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "seen_idx = [24,15,6,14,2]\n",
    "unseen_idx = [0]\n",
    "\n",
    "num_unseen = len(unseen_idx)\n",
    "num_seen = len(seen_idx)\n",
    "\n",
    "# def one-hot index\n",
    "def idxtoOneHot(idx, length):\n",
    "  label_ohe = np.zeros((1,length))\n",
    "  label_ohe[0][idx] = 1\n",
    "  return label_ohe\n",
    "\n",
    "# def merge dictionary\n",
    "def Merge(dict1, dict2):\n",
    "  res = {**dict1, **dict2}\n",
    "  return res\n",
    "\n",
    "\n",
    "# def dictionary for unseen, seen, and name\n",
    "def generate_label_dict(unseen_idx, seed_idx):\n",
    "\n",
    "  \"\"\"\n",
    "    generate the label of each device and its corresponding name\n",
    "\n",
    "    like {9: 'iHome'}, we shuffle the data, 9 --> 10\n",
    "    the data will be  {'iHome': 10}\n",
    "\n",
    "    \"\"\"\n",
    "  unseen_dict = dict()\n",
    "  seen_dict = dict()\n",
    "  all_index_to_name = dict()\n",
    "  seen_index_to_name = dict()\n",
    "  unseen_index_to_name = dict()\n",
    "  for i in range(0,num_seen):\n",
    "    seen_dict[seen_idx[i]] = i\n",
    "    # seen_dict[seen_idx[i]] = 0   ## only 2 modules\n",
    "    seen_index_to_name[i] = all_device_names_dict[seen_idx[i]]\n",
    "  for i in range(0,num_unseen):\n",
    "    unseen_dict[unseen_idx[i]] = num_seen + i\n",
    "    # unseen_dict[unseen_idx[i]] = 1\n",
    "    unseen_index_to_name[num_seen+i] = all_device_names_dict[unseen_idx[i]]\n",
    "  total_dict = Merge(unseen_dict, seen_dict)\n",
    "  all_index_to_name = Merge(seen_index_to_name, unseen_index_to_name)\n",
    "  return unseen_dict, seen_dict, total_dict,all_index_to_name\n",
    "\n",
    "## generate the training data\n",
    "def seen_training_data(init_x_train, init_x_test, init_y_train, init_y_test, seen_dict):\n",
    "  x_train_feature = []\n",
    "  y_train_feature = []\n",
    "  x_test_feature = []\n",
    "  y_test_feature = []\n",
    "\n",
    "  ## change from one-hot to label\n",
    "  for i in range(len(init_y_train)):\n",
    "    index = np.where(init_y_train[i][0] == True)\n",
    "    k = index[0][0]\n",
    "    if k in seen_idx:\n",
    "      x_train_feature.append(init_x_train[i])\n",
    "      # idx to new range\n",
    "      new_k = seen_dict[k]\n",
    "      y_train_feature.append(idxtoOneHot(new_k,len(seen_idx)))\n",
    "\n",
    "  for i in range(len(init_y_test)):\n",
    "    index = np.where(init_y_test[i][0] == True)\n",
    "    k = index[0][0]\n",
    "    if k in seen_idx:\n",
    "      x_test_feature.append(init_x_test[i])\n",
    "      # idx to new range\n",
    "      new_k = seen_dict[k]\n",
    "      y_test_feature.append(idxtoOneHot(new_k,len(seen_idx)))\n",
    "  return np.array(x_train_feature), np.array(y_train_feature), np.array(x_test_feature), np.array(y_test_feature)\n",
    "\n",
    "\n",
    "\n",
    "## extract features for the final test with both seen and unseen data\n",
    "def feature_extraction_data_seen_and_unseen(init_x_train, init_x_test, init_y_train, init_y_test, attr_dict):\n",
    "\n",
    "  x_train_attr = []\n",
    "  y_train_attr = []\n",
    "  x_test_attr = []\n",
    "  y_test_attr = []\n",
    "\n",
    "  attr_idx = [24,15,6,14,2,0]\n",
    "  for i in range(len(init_y_train)):\n",
    "    index = np.where(init_y_train[i][0] == True)\n",
    "    k = index[0][0]\n",
    "    if k in attr_idx:\n",
    "      x_train_attr.append(init_x_train[i])\n",
    "      # idx to new range\n",
    "      new_k = attr_dict[k]\n",
    "      y_train_attr.append(idxtoOneHot(new_k,len(attr_idx)))\n",
    "\n",
    "  for i in range(len(init_y_test)):\n",
    "    index = np.where(init_y_test[i][0] == True)\n",
    "    k = index[0][0]\n",
    "    if k in attr_idx:\n",
    "      x_test_attr.append(init_x_test[i])\n",
    "      # idx to new range\n",
    "      new_k = attr_dict[k]\n",
    "      y_test_attr.append(idxtoOneHot(new_k,len(attr_idx)))\n",
    "\n",
    "  x_train_attr = np.array(x_train_attr)\n",
    "  y_train_attr = np.array(y_train_attr)\n",
    "  x_test_attr = np.array(x_test_attr)\n",
    "  y_test_attr = np.array(y_test_attr)\n",
    "\n",
    "  return x_train_attr,  y_train_attr, x_test_attr, y_test_attr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "63e474c6-0405-4694-8d1a-fb394133be10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unseen dictionary {0: 5}\n",
      "seen dictionary {24: 0, 15: 1, 6: 2, 14: 3, 2: 4}\n",
      "total dictionary {0: 5, 24: 0, 15: 1, 6: 2, 14: 3, 2: 4}\n",
      "index to name dictionary {0: 'Android Phone', 1: 'Light Bulbs LiFX Smart Bulb', 2: 'Withings Smart Baby Monitor', 3: 'Withings Aura smart sleep sensor', 4: 'Netatmo Welcome', 5: 'Smart Things'}\n",
      "shape of train windws: (100049, 200, 8)\n",
      "shape of train labels: (100049, 1, 6)\n",
      "shape of test windows: (25208, 200, 8)\n",
      "shape of test labels: (25208, 1, 6)\n",
      "shape of train windws: (70837, 200, 8)\n",
      "shape of train labels: (70837, 1, 5)\n",
      "shape of test windows: (17966, 200, 8)\n",
      "shape of test labels: (17966, 1, 5)\n"
     ]
    }
   ],
   "source": [
    "unseen_dict, seen_dict,total_dict,all_index_to_name = generate_label_dict(unseen_idx, seen_idx)\n",
    "x_train_feature, y_train_feature, x_test_feature, y_test_feature = seen_training_data(x_train, x_test, y_train, y_test, seen_dict)\n",
    "x_train_attr,  y_train_attr, x_test_attr, y_test_attr = feature_extraction_data_seen_and_unseen(x_train, x_test, y_train, y_test, total_dict)\n",
    "\n",
    "print('unseen dictionary', unseen_dict)\n",
    "print('seen dictionary', seen_dict)\n",
    "print('total dictionary', total_dict)\n",
    "print('index to name dictionary', all_index_to_name)\n",
    "\n",
    "print(\"shape of train windws: {}\".format(x_train_attr.shape))\n",
    "print(\"shape of train labels: {}\".format(y_train_attr.shape))\n",
    "print(\"shape of test windows: {}\".format(x_test_attr.shape))\n",
    "print(\"shape of test labels: {}\".format(y_test_attr.shape))\n",
    "\n",
    "print(\"shape of train windws: {}\".format(x_train_feature.shape))\n",
    "print(\"shape of train labels: {}\".format(y_train_feature.shape))\n",
    "print(\"shape of test windows: {}\".format(x_test_feature.shape))\n",
    "print(\"shape of test labels: {}\".format(y_test_feature.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eecb59e9-471f-48ab-a3ab-6dddf4958339",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train, x_test, y_test = x_train_attr,  y_train_attr, x_test_attr, y_test_attr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "213c704c-9b88-4302-92d7-6264fd4aac0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change label from one-hot to integer\n",
    "y_train_labels = []\n",
    "y_test_labels = []\n",
    "for i in range(len(y_train)):\n",
    "  index = np.where(y_train[i][0] == True)\n",
    "  k = index[0][0]\n",
    "  y_train_labels.append(k)\n",
    "\n",
    "for i in range(len(y_test)):\n",
    "  index = np.where(y_test[i][0] == True)\n",
    "  k = index[0][0]\n",
    "  y_test_labels.append(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f7b0d289-b114-4275-a891-a7da4e1cf033",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dataset(data):\n",
    "    \"\"\"\n",
    "    Process the dataset by applying the following rules:\n",
    "    1. Cap values of features 6 and 7.\n",
    "    2. Bin the continuous features 0 and 3 into 1026 bins.\n",
    "    3. Change any -1 values to 0 for all features.\n",
    "\n",
    "    Parameters:\n",
    "    data (numpy.ndarray): The input dataset with shape (n_samples, 200, 8).\n",
    "\n",
    "    Returns:\n",
    "    numpy.ndarray: The processed dataset.\n",
    "    \"\"\"\n",
    "    # Rule 1: Cap values for features 6 and 7\n",
    "    data[:, :, 6] = np.where(data[:, :, 6] > 1024, 1025, data[:, :, 6])\n",
    "    data[:, :, 7] = np.where(data[:, :, 7] > 1024, 1025, data[:, :, 7])\n",
    "\n",
    "    # Rule 2: Bin the continuous features at indices 0 and 3\n",
    "    num_bins = 1026\n",
    "\n",
    "    # Get the min and max values for each feature to define the bins\n",
    "    min_val_0, max_val_0 = data[:, :, 0].min(), data[:, :, 0].max()\n",
    "    min_val_3, max_val_3 = data[:, :, 3].min(), data[:, :, 3].max()\n",
    "\n",
    "    # Create the bin edges\n",
    "    bins_0 = np.linspace(min_val_0, max_val_0, num_bins + 1)\n",
    "    bins_3 = np.linspace(min_val_3, max_val_3, num_bins + 1)\n",
    "\n",
    "    # Bin the data\n",
    "    data[:, :, 0] = np.digitize(data[:, :, 0], bins_0) - 1  # Bin and adjust to 0-based indexing\n",
    "    data[:, :, 3] = np.digitize(data[:, :, 3], bins_3) - 1\n",
    "\n",
    "    # Convert to category features (ensure integer type)\n",
    "    data[:, :, 0] = data[:, :, 0].astype(int)\n",
    "    data[:, :, 3] = data[:, :, 3].astype(int)\n",
    "\n",
    "    # Rule 3: Change any -1 values to 0 for all features\n",
    "    data = np.where(data == -1, 0, data)\n",
    "\n",
    "    return data\n",
    "\n",
    "# Example usage\n",
    "# Assuming 'your_dataset' is a numpy array with shape (1071459, 200, 8)\n",
    "# processed_data = process_dataset(your_dataset)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "edb4315e-e47c-4ff2-aa36-82200575a18c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = process_dataset(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1a9cbc46-b246-471e-a608-6483d68ff1ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = process_dataset(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9df48762-a5a8-4aa9-b7e2-60365ca704f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 55.,   6.,   1.,   0.,   1.,   0.,   0., 443.],\n",
       "       [ 55.,   6.,   0.,   0.,   0.,   1., 443.,   0.]], dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[1][0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8eb7215c-8ea4-4795-8104-4f3ea3bbc1fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_data_and_add_paddings(data):\n",
    "    # Flatten and reshape\n",
    "    flattened_data = data.reshape(data.shape[0], -1)  # Shape becomes (1071459, 1600)\n",
    "    # Step 2: Reshape to add a new dimension at the end\n",
    "    reshaped_data = flattened_data.reshape(data.shape[0], -1)  # Shape becomes (1071459,  1600)\n",
    "    new_length = 2000\n",
    "    # Step 1: Create a new array of zeros with the desired shape\n",
    "    padded_data = np.zeros((reshaped_data.shape[0], new_length))\n",
    "    # Step 2: Copy the original data into the new array\n",
    "    padded_data[:, :reshaped_data.shape[1]] = reshaped_data\n",
    "    return padded_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "469bbc2e-d5a6-45d6-a96f-aaefec08ac34",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_new_test = flatten_data_and_add_paddings(x_test)\n",
    "x_new_train = flatten_data_and_add_paddings(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e64b5847-8f77-45b6-adab-585d501fe2a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_labels = np.array(y_train_labels)\n",
    "y_test_labels = np.array(y_test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "848c77e8-6cd6-44df-b18b-6b4ceae59a12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4, 5])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(y_train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3d8ca54b-45e0-4d6a-b33d-16247d34a1e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of train windws: (100049, 2000)\n",
      "shape of train labels: (100049,)\n",
      "shape of test windows: (25208, 2000)\n",
      "shape of test labels: (25208,)\n"
     ]
    }
   ],
   "source": [
    "print(\"shape of train windws: {}\".format(x_new_train.shape))\n",
    "print(\"shape of train labels: {}\".format(y_train_labels.shape))\n",
    "print(\"shape of test windows: {}\".format(x_new_test.shape))\n",
    "print(\"shape of test labels: {}\".format(y_test_labels.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "60b81da7-ac02-4332-9060-30b9c8591cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_flow_packet_embedding(data):\n",
    "    # Generate the array filled with ones\n",
    "    ones_array = np.ones(data.shape)\n",
    "    \n",
    "    # Print the shape to verify\n",
    "    print(\"Generated array shape:\", ones_array.shape)\n",
    "    return ones_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "74121d74-7bcb-41ca-aa4d-2e595f174463",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_segments = np.ones((25208,2000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "24a9630c-5619-45cf-b2ef-66527b6919d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_segments = np.ones((100049,2000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a067a964-6b4b-4a5f-b6e5-0776684cac43",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetformerDatasetDownstream(Dataset):\n",
    "    def __init__(self, input_sequences, input_labels, input_segments, seq_len = 2000):\n",
    "        self.seq_len = seq_len\n",
    "        self.session_flows = len(input_sequences)\n",
    "        self.sessions = input_sequences\n",
    "        self.segments = input_segments\n",
    "        self.labels = input_labels\n",
    "        self.special_token_dict =  {'PAD': 0, 'MASK': 1028}\n",
    "        self.mask_ratio = 0\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.session_flows\n",
    "\n",
    "    def __getitem__(self,item):\n",
    "\n",
    "        ##step 1 : get random sessions \n",
    "        s1, seg1,seq_label = self.get_session_flow(item)\n",
    "\n",
    "        ## step 2: replace random word in sentence \n",
    "        s1_random, s1_label, s1_idx = self.random_word(s1)\n",
    "        \n",
    "        segment_label = seg1\n",
    "\n",
    "        netformer_input = s1_random\n",
    "        netformer_label = s1_label\n",
    "        netformer_idx = s1_idx\n",
    "\n",
    "        \n",
    "        output = {\"netformer_input\": netformer_input,\n",
    "                  \"netformer_label\": netformer_label,\n",
    "                  \"netformer_idx\":netformer_idx,\n",
    "                  \"segment_label\": segment_label,\n",
    "                \"sequence_label\": seq_label}\n",
    "\n",
    "        return {key: torch.tensor(value,dtype=torch.float32) for key, value in output.items()}\n",
    "\n",
    "\n",
    "    def random_word(self, sentence):\n",
    "        output_label = []\n",
    "        output = []\n",
    "        output_idx =[]\n",
    "\n",
    "\n",
    "        for i, token in enumerate(sentence):\n",
    "            prob = random.random()\n",
    "\n",
    "            if prob < self.mask_ratio:\n",
    "                prob /= self.mask_ratio\n",
    "    \n",
    "                if prob < 0.8:\n",
    "                    output.append(self.special_token_dict['MASK'])\n",
    "                elif prob < 0.9:\n",
    "                    output.append(self.random_selection(self.sessions))\n",
    "                else:\n",
    "                    output.append(token)\n",
    "    \n",
    "                output_label.append(token)\n",
    "                output_idx.append(1)\n",
    "    \n",
    "            else:\n",
    "                output.append(token)\n",
    "                output_label.append(0)\n",
    "                output_idx.append(0)\n",
    "                \n",
    "\n",
    "        assert len(output) == len(output_label)\n",
    "        return output, output_label, output_idx\n",
    "        \n",
    "\n",
    "    def random_selection(self, input_sequences):\n",
    "        rand_session = random.randrange(len(input_sequences))\n",
    "        rand_flow = random.randrange(len(input_sequences[rand_session]))\n",
    "        return input_sequences[rand_session][rand_flow]\n",
    "        \n",
    "\n",
    "    def get_session_flow(self, item):\n",
    "        '''Return session data and segments'''\n",
    "        return self.sessions[item], self.segments[item],self.labels[item]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1fb8a35f-7750-4e07-af7b-4af8c1b78c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = NetformerDatasetDownstream(x_new_train,y_train_labels,x_train_segments, seq_len=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "df3c4f0d-c5f6-4d8d-a8f6-6caf122256e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_data, batch_size=4, shuffle=True, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ff810b63-6ce1-4e20-83eb-d88516d62942",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_data = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5f5c3daa-23f7-4f4d-aa77-4ed98a297652",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PositionalEmbedding(10,2000)(sample_data).shape\n",
    "# a = sample_data['netformer_input']\n",
    "# b = sample_data['segment_label']\n",
    "# print(a.shape)\n",
    "# print(b.shape)\n",
    "# # Convert b to data type torch.long\n",
    "# b = b.to(torch.long)\n",
    "# embeddings = NetformerEmbedding(1030, 10)(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df72ab79-cb51-41c2-850d-0b753f536880",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Parameters: 24957\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:0:   0%|| 3/25013 [00:00<51:58,  8.02it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 0, 'iter': 0, 'loss': 1.6681149005889893}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:0:  40%|| 10003/25013 [11:33<17:22, 14.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 0, 'iter': 10000, 'loss': 0.001857154187746346}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:0:  50%|| 12545/25013 [14:29<14:21, 14.46it/s]"
     ]
    }
   ],
   "source": [
    "# device = 'cpu'\n",
    "NetFormer_model = NetFormer(\n",
    "  feature_size = 1030,\n",
    "  d_model=10,\n",
    "  n_layers=2,\n",
    "  heads=10,\n",
    "  dropout=0.1,\n",
    "number_of_class=6,\n",
    ").to(device)\n",
    "\n",
    "\n",
    "# NetFormer_model.to(device)\n",
    "net_lm = NetFormerLM(NetFormer_model, number_of_class=6).to(device)\n",
    "# # bert_lm.to(device)\n",
    "net_trainer = NetformerTrainer(net_lm, train_loader,device = 'cuda')\n",
    "epochs = 10\n",
    "\n",
    "for epoch in range(epochs):\n",
    "  net_trainer.train(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10370498-6578-4bdf-bbc0-fa8f9d1a7ea6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0af35a-41ac-4973-b6f3-2a51a1a7b40c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
